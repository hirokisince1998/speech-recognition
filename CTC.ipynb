{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CTC.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"29M_dLn2BMC9","colab_type":"text"},"cell_type":"markdown","source":["# CTCを用いた可変長音素認識\n","\n","これまでの検討では、訓練用データに時間情報付きの音素ラベルが与えられていることが前提でした。しかし、ほとんどの音声データベースには時間情報がなく、発話内容しか与えられていません。これは、音素の時間情報を同定する作業が非常に高コストだからです。\n","\n","このため、ニューラルネットを用いた音声認識は、これまでは学習時にHMM(隠れマルコフモデル)を用いた従来の音声認識を援用し、はじめに各音素の区間を音声認識によって推定する必要がありました。\n","\n","これに対し、近年ニューラルネットだけで音声認識を可能とする枠組がいくつか提案されています。CTC (Connectionist Temporal Classification) は、そのようなアルゴリズムの1つです。\n","\n","CTC は、基本的には LSTM を用いたフレームごとの音素認識に過ぎません。しかし、損失関数が異なります。CTC では、縮約すると正解音素列と一致するような全ての音素列の確率の和をネットワークの出力の「望ましさ」と考え、その対数にマイナスを付けた関数を損失関数とします。例えば、縮約すると /h a i/ という音素列と一致するような長さ5の音素列を全て挙げると"]},{"metadata":{"id":"-sWoq0ukBMC_","colab_type":"code","colab":{}},"cell_type":"code","source":["from itertools import combinations, combinations_with_replacement\n","import numpy as np\n","\n","def expandstr(str, seqlen):\n","    l = []\n","    for expdstrlen in range(len(str), seqlen+1):\n","        combos = combinations(range(1, expdstrlen), len(str)-1)\n","        for sp in combos:\n","            tmp = np.array(sp)\n","            clens = np.append(tmp,expdstrlen) - np.insert(tmp,0,0)\n","            bstr = \"\".join([c * clens[i] for i, c in enumerate(str)])\n","            bcombos = combinations_with_replacement(range(0,len(str)+1), seqlen-expdstrlen)\n","            for bcombo in bcombos:\n","                xstr = \"\"\n","                for i in range(len(str)+1):\n","                    xstr += '_' * bcombo.count(i)\n","                    if i < len(str):\n","                        xstr += str[i] * clens[i]\n","                l.append(xstr)\n","    return l"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ozXTEI9gBMDC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":493},"outputId":"964d3584-ac02-4948-ae3d-ca969210d7b2","executionInfo":{"status":"ok","timestamp":1534998198654,"user_tz":-540,"elapsed":743,"user":{"displayName":"Hiroki Mori","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109920332067293067651"}}},"cell_type":"code","source":["expandstr('hai',5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__hai',\n"," '_h_ai',\n"," '_ha_i',\n"," '_hai_',\n"," 'h__ai',\n"," 'h_a_i',\n"," 'h_ai_',\n"," 'ha__i',\n"," 'ha_i_',\n"," 'hai__',\n"," '_haii',\n"," 'h_aii',\n"," 'ha_ii',\n"," 'haii_',\n"," '_haai',\n"," 'h_aai',\n"," 'haa_i',\n"," 'haai_',\n"," '_hhai',\n"," 'hh_ai',\n"," 'hha_i',\n"," 'hhai_',\n"," 'haiii',\n"," 'haaii',\n"," 'haaai',\n"," 'hhaii',\n"," 'hhaai',\n"," 'hhhai']"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"Np_wzFLRBMDI","colab_type":"text"},"cell_type":"markdown","source":["となります。ただし _ はどの音素でもない記号で、ブランクと呼びます。\n","\n","以下、ネットワークの定義はLSTMの時と同じです。"]},{"metadata":{"id":"qLeoAT_dBMDJ","colab_type":"code","colab":{}},"cell_type":"code","source":["import chainer\n","import chainer.links as L\n","import chainer.functions as F\n","\n","class RNN(chainer.Chain):\n","\n","    def __init__(self, n_lstm_layers=1, n_mid_units=100, n_out=41, dropout=0.2):\n","        super(RNN, self).__init__()\n","\n","        # パラメータを持つ層の登録\n","        with self.init_scope():\n","            self.l1 = L.Linear(None, n_mid_units)\n","            self.l2 = L.NStepBiLSTM(n_lstm_layers, n_mid_units, n_mid_units, dropout)\n","            self.l3 = L.Linear(n_mid_units * 2, n_out)\n","\n","    def __call__(self, x):\n","        # データを受け取った際のforward計算を書く\n","        h1 = [ F.relu(self.l1(X)) for X in x ]\n","        hy, cy, ys = self.l2(None, None, h1)\n","        h2 = F.concat(ys, axis=0)\n","        return self.l3(h2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xtEwW9GVBMDN","colab_type":"code","colab":{}},"cell_type":"code","source":["from chainer.dataset import to_device\n","from chainer.dataset import concat_examples\n","import numpy as np\n","\n","def converter(batch, device):\n","    # alternative to chainer.dataset.concat_examples\n","    \n","    Xs = [to_device(device, X) for X, _, __ in batch]\n","    ts = [to_device(device, t) for _, t, __ in batch]\n","\n","    lab_batch = [lab.astype(np.int32) for _, __, lab in batch]\n","    labs = concat_examples(lab_batch, device, padding=0)\n","    \n","    return Xs, ts, labs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k2jXr7cJBMDQ","colab_type":"text"},"cell_type":"markdown","source":["## 損失関数\n","\n","CTCの損失関数は、`connectionist_temporal_classification`関数で計算できます。この関数は、これまでのようにフレームごとでなく発話全体に対して計算されます。各フレームの正解音素ラベル `t` を必要とせず、発話全体の音素ラベル列 `lab` さえあればよい点が重要なポイントです。"]},{"metadata":{"id":"2OszfJxmBMDS","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from chainer.backends import cuda\n","\n","def calculate_loss(model, batch, blank_symbol, gpu_id=0, train_mode=True):\n","    numframes = [X.shape[0] for (X, t, lab) in batch]\n","    \n","    # lab を行列に変形\n","    label_length = xp.array([len(lab) for (X, _t, lab) in batch],dtype=np.int32)\n","    Xs, _, labs = converter(batch, gpu_id) # アラインメント済ラベルは使わない\n","\n","    with chainer.using_config('train', train_mode), \\\n","         chainer.using_config('enable_backprop', train_mode):\n","        ys = net(Xs)\n","    \n","    # 発話ごとに分割\n","    split_point = np.cumsum(numframes)[:-1]\n","    y4utt = F.split_axis(ys, split_point, axis=0)\n","    input_length = xp.array(numframes, dtype=np.int32)\n","    \n","    # 第iフレームの確率行列(B,V)となるよう整形\n","    y4utt = F.pad_sequence(y4utt)\n","    y4frame = F.stack(y4utt, axis=1)\n","    x = [xi for xi in y4frame]\n","    \n","    # ロスの計算\n","    loss = F.connectionist_temporal_classification(x, labs, blank_symbol, input_length, label_length)\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ybkq0C0uBMDW","colab_type":"code","colab":{}},"cell_type":"code","source":["import random\n","import numpy as np\n","import chainer\n","\n","def reset_seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    if gpu_id >= 0:\n","        chainer.cuda.cupy.random.seed(seed)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n-Q8IannBMDa","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","with open('phones') as f:\n","    phones = f.read().splitlines()\n","le = LabelEncoder()\n","le.fit(phones)\n","blank_symbol = np.asscalar(le.transform(['_'])[0])\n","nsymbol = len(phones)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5PXTzZC5BMDd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3417},"outputId":"dbbb5799-2491-40b7-a216-0f0da666f932","executionInfo":{"status":"ok","timestamp":1534999711849,"user_tz":-540,"elapsed":1507191,"user":{"displayName":"Hiroki Mori","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109920332067293067651"}}},"cell_type":"code","source":["import numpy as np\n","from chainer import iterators\n","from chainer import optimizers\n","from chainer.backends import cuda\n","from chainer.cuda import to_cpu\n","from chainer.dataset import concat_examples\n","\n","gpu_id = 0 # CPUを用いる場合は、この値を-1にしてください\n","\n","batchsize = 100\n","max_epoch = 200\n","\n","xp = np\n","if gpu_id >= 0:\n","    xp = chainer.cuda.cupy\n","\n","reset_seed(1234) # 乱数の種をセット\n","\n","train = np.load(\"MHT-train.npy\") # train: 450 x (framelen x 26 , framelen, maxphonenum)\n","test = np.load(\"MHT-test.npy\")\n","\n","train_iter = iterators.SerialIterator(train, batchsize)\n","\n","# dropoutを大きくしないと、訓練データの音素列に過剰に適合するようだ\n","net = RNN(n_lstm_layers=1, n_mid_units=200, n_out=nsymbol, dropout=0.4)\n","\n","#optimizer = optimizers.SGD(lr=0.0001).setup(net)\n","optimizer = optimizers.Adam().setup(net)\n","\n","if gpu_id >= 0:\n","    net.to_gpu(gpu_id)\n","    \n","while train_iter.epoch < max_epoch:\n","\n","    # ---------- 学習の1イテレーション ----------\n","    train_batch = train_iter.next()\n","\n","    loss = calculate_loss(net, train_batch, blank_symbol, gpu_id, train_mode = True)\n","\n","    # 勾配の計算\n","    net.cleargrads()\n","    loss.backward()\n","\n","    # バッチ単位で古い記憶を削除し、計算コストを削減する。\n","    loss.unchain_backward()\n","\n","    # パラメータの更新\n","    optimizer.update()\n","    \n","    # --------------- ここまで ----------------\n","\n","    # 1エポック終了ごとにValidationデータに対する予測精度を測って、\n","    # モデルの汎化性能が向上していることをチェックしよう\n","    if train_iter.is_new_epoch:  # 1 epochが終わったら\n","        # ロスの表示\n","        print('epoch:{:02d} train_loss:{:.04f} '.format(\n","            train_iter.epoch, float(to_cpu(loss.data))), end='')\n","\n","        valid_losses = []\n","        \n","        loss_valid = calculate_loss(net, test, blank_symbol, gpu_id, train_mode=False)\n","        valid_losses.append(to_cpu(loss_valid.array))\n","\n","        print('val_loss:{:.04f}'.format(np.mean(valid_losses)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch:01 train_loss:541.2224 val_loss:401.2343\n","epoch:02 train_loss:470.5587 val_loss:428.6295\n","epoch:03 train_loss:427.5812 val_loss:344.4819\n","epoch:04 train_loss:421.0080 val_loss:328.5205\n","epoch:05 train_loss:407.3997 val_loss:335.0782\n","epoch:06 train_loss:386.1239 val_loss:335.0728\n","epoch:07 train_loss:396.0115 val_loss:322.8152\n","epoch:08 train_loss:404.2352 val_loss:316.6690\n","epoch:09 train_loss:399.6893 val_loss:312.7981\n","epoch:10 train_loss:383.7571 val_loss:311.9659\n","epoch:11 train_loss:356.4778 val_loss:308.5427\n","epoch:12 train_loss:371.0016 val_loss:307.5361\n","epoch:13 train_loss:335.9829 val_loss:303.8176\n","epoch:14 train_loss:356.9879 val_loss:297.6185\n","epoch:15 train_loss:366.5684 val_loss:296.2573\n","epoch:16 train_loss:350.5456 val_loss:294.5177\n","epoch:17 train_loss:331.8772 val_loss:293.4913\n","epoch:18 train_loss:346.9295 val_loss:289.7409\n","epoch:19 train_loss:327.4168 val_loss:285.9658\n","epoch:20 train_loss:345.1972 val_loss:287.1277\n","epoch:21 train_loss:357.7995 val_loss:278.7282\n","epoch:22 train_loss:323.0463 val_loss:275.9292\n","epoch:23 train_loss:311.1105 val_loss:273.1131\n","epoch:24 train_loss:285.7404 val_loss:264.0275\n","epoch:25 train_loss:299.4994 val_loss:253.6127\n","epoch:26 train_loss:281.0672 val_loss:262.6609\n","epoch:27 train_loss:285.2067 val_loss:241.2246\n","epoch:28 train_loss:269.9069 val_loss:249.1761\n","epoch:29 train_loss:254.3921 val_loss:235.6274\n","epoch:30 train_loss:282.7871 val_loss:241.0485\n","epoch:31 train_loss:256.7698 val_loss:227.6515\n","epoch:32 train_loss:252.6376 val_loss:218.8144\n","epoch:33 train_loss:253.4857 val_loss:231.1888\n","epoch:34 train_loss:248.7635 val_loss:210.3142\n","epoch:35 train_loss:237.9399 val_loss:214.6060\n","epoch:36 train_loss:256.2050 val_loss:205.1592\n","epoch:37 train_loss:257.5809 val_loss:172.2962\n","epoch:38 train_loss:238.6756 val_loss:153.4571\n","epoch:39 train_loss:198.1624 val_loss:134.1129\n","epoch:40 train_loss:187.9570 val_loss:135.3976\n","epoch:41 train_loss:192.3903 val_loss:132.9107\n","epoch:42 train_loss:184.2011 val_loss:131.6567\n","epoch:43 train_loss:190.2954 val_loss:129.4136\n","epoch:44 train_loss:191.2623 val_loss:128.0382\n","epoch:45 train_loss:186.2559 val_loss:126.5215\n","epoch:46 train_loss:176.3450 val_loss:124.9119\n","epoch:47 train_loss:174.3728 val_loss:122.4286\n","epoch:48 train_loss:171.7331 val_loss:120.0182\n","epoch:49 train_loss:166.0157 val_loss:116.8052\n","epoch:50 train_loss:161.8395 val_loss:114.0663\n","epoch:51 train_loss:160.3094 val_loss:110.6878\n","epoch:52 train_loss:159.6646 val_loss:107.7612\n","epoch:53 train_loss:156.1887 val_loss:104.2237\n","epoch:54 train_loss:145.7383 val_loss:101.3184\n","epoch:55 train_loss:139.9064 val_loss:97.7789\n","epoch:56 train_loss:133.6842 val_loss:95.0444\n","epoch:57 train_loss:130.6186 val_loss:91.7297\n","epoch:58 train_loss:127.6113 val_loss:89.2090\n","epoch:59 train_loss:124.4244 val_loss:86.3322\n","epoch:60 train_loss:116.9524 val_loss:84.0469\n","epoch:61 train_loss:112.0973 val_loss:81.0716\n","epoch:62 train_loss:117.3088 val_loss:79.0982\n","epoch:63 train_loss:107.7403 val_loss:76.5876\n","epoch:64 train_loss:110.9275 val_loss:74.3501\n","epoch:65 train_loss:107.4540 val_loss:71.6642\n","epoch:66 train_loss:95.6593 val_loss:69.8085\n","epoch:67 train_loss:94.4479 val_loss:67.3839\n","epoch:68 train_loss:90.1129 val_loss:65.7627\n","epoch:69 train_loss:94.5985 val_loss:63.4287\n","epoch:70 train_loss:86.8206 val_loss:61.7427\n","epoch:71 train_loss:84.5262 val_loss:59.9196\n","epoch:72 train_loss:81.7180 val_loss:58.1413\n","epoch:73 train_loss:80.4027 val_loss:56.4066\n","epoch:74 train_loss:78.8740 val_loss:54.9475\n","epoch:75 train_loss:79.1268 val_loss:53.1596\n","epoch:76 train_loss:74.3045 val_loss:51.9488\n","epoch:77 train_loss:74.8208 val_loss:50.0934\n","epoch:78 train_loss:71.1005 val_loss:48.9079\n","epoch:79 train_loss:66.2416 val_loss:47.0220\n","epoch:80 train_loss:65.8564 val_loss:45.9252\n","epoch:81 train_loss:59.2284 val_loss:44.4646\n","epoch:82 train_loss:63.1478 val_loss:43.3867\n","epoch:83 train_loss:57.0387 val_loss:41.7865\n","epoch:84 train_loss:52.5468 val_loss:40.8556\n","epoch:85 train_loss:55.6095 val_loss:39.7144\n","epoch:86 train_loss:52.2265 val_loss:38.5610\n","epoch:87 train_loss:50.4001 val_loss:37.7981\n","epoch:88 train_loss:51.4497 val_loss:36.9512\n","epoch:89 train_loss:49.3964 val_loss:36.1057\n","epoch:90 train_loss:48.0595 val_loss:35.3098\n","epoch:91 train_loss:45.2293 val_loss:34.1827\n","epoch:92 train_loss:45.6875 val_loss:33.4290\n","epoch:93 train_loss:41.7954 val_loss:32.9570\n","epoch:94 train_loss:41.3651 val_loss:32.3361\n","epoch:95 train_loss:40.9315 val_loss:31.2787\n","epoch:96 train_loss:39.1193 val_loss:30.9952\n","epoch:97 train_loss:38.5577 val_loss:30.1917\n","epoch:98 train_loss:36.7469 val_loss:29.4052\n","epoch:99 train_loss:36.4474 val_loss:28.7909\n","epoch:100 train_loss:36.6564 val_loss:28.7937\n","epoch:101 train_loss:34.1338 val_loss:28.0759\n","epoch:102 train_loss:35.4423 val_loss:27.7101\n","epoch:103 train_loss:35.2985 val_loss:27.0177\n","epoch:104 train_loss:31.3942 val_loss:26.5811\n","epoch:105 train_loss:30.6400 val_loss:26.0230\n","epoch:106 train_loss:31.5859 val_loss:25.4798\n","epoch:107 train_loss:31.1980 val_loss:24.9956\n","epoch:108 train_loss:30.2525 val_loss:24.7203\n","epoch:109 train_loss:28.9848 val_loss:23.8906\n","epoch:110 train_loss:26.8722 val_loss:23.5732\n","epoch:111 train_loss:26.9507 val_loss:23.4278\n","epoch:112 train_loss:26.7492 val_loss:22.9242\n","epoch:113 train_loss:26.2422 val_loss:22.4269\n","epoch:114 train_loss:24.9347 val_loss:22.2666\n","epoch:115 train_loss:23.8152 val_loss:21.7972\n","epoch:116 train_loss:24.5740 val_loss:21.6242\n","epoch:117 train_loss:24.9086 val_loss:21.1940\n","epoch:118 train_loss:22.3575 val_loss:21.0940\n","epoch:119 train_loss:21.3467 val_loss:20.7463\n","epoch:120 train_loss:21.2056 val_loss:20.4143\n","epoch:121 train_loss:21.3319 val_loss:20.0120\n","epoch:122 train_loss:20.3746 val_loss:20.2013\n","epoch:123 train_loss:19.5876 val_loss:19.9975\n","epoch:124 train_loss:19.2270 val_loss:19.4274\n","epoch:125 train_loss:19.9648 val_loss:19.1521\n","epoch:126 train_loss:18.3063 val_loss:18.9522\n","epoch:127 train_loss:18.2260 val_loss:18.9148\n","epoch:128 train_loss:17.7208 val_loss:18.8805\n","epoch:129 train_loss:17.0989 val_loss:18.5134\n","epoch:130 train_loss:17.4637 val_loss:18.3804\n","epoch:131 train_loss:15.7940 val_loss:18.4153\n","epoch:132 train_loss:15.6969 val_loss:18.0022\n","epoch:133 train_loss:15.2252 val_loss:17.9852\n","epoch:134 train_loss:15.7271 val_loss:17.6484\n","epoch:135 train_loss:15.0516 val_loss:17.5312\n","epoch:136 train_loss:15.0476 val_loss:17.2903\n","epoch:137 train_loss:15.4103 val_loss:17.3282\n","epoch:138 train_loss:13.7997 val_loss:16.6720\n","epoch:139 train_loss:13.6245 val_loss:16.7800\n","epoch:140 train_loss:13.2403 val_loss:17.2475\n","epoch:141 train_loss:12.4195 val_loss:16.3853\n","epoch:142 train_loss:12.9009 val_loss:16.4601\n","epoch:143 train_loss:11.0477 val_loss:16.3725\n","epoch:144 train_loss:12.4645 val_loss:16.2767\n","epoch:145 train_loss:11.5035 val_loss:16.0418\n","epoch:146 train_loss:11.3511 val_loss:15.6195\n","epoch:147 train_loss:10.7561 val_loss:16.1444\n","epoch:148 train_loss:10.8753 val_loss:15.5683\n","epoch:149 train_loss:11.4167 val_loss:15.6512\n","epoch:150 train_loss:11.3131 val_loss:15.6840\n","epoch:151 train_loss:9.5908 val_loss:15.6917\n","epoch:152 train_loss:10.3163 val_loss:15.4944\n","epoch:153 train_loss:9.3377 val_loss:15.6832\n","epoch:154 train_loss:9.3158 val_loss:15.2554\n","epoch:155 train_loss:8.9508 val_loss:15.2082\n","epoch:156 train_loss:9.0269 val_loss:15.1560\n","epoch:157 train_loss:8.7691 val_loss:15.0429\n","epoch:158 train_loss:8.5937 val_loss:15.3299\n","epoch:159 train_loss:8.7067 val_loss:15.2336\n","epoch:160 train_loss:8.1248 val_loss:14.8240\n","epoch:161 train_loss:7.7521 val_loss:15.0037\n","epoch:162 train_loss:8.1031 val_loss:15.1448\n","epoch:163 train_loss:7.4061 val_loss:15.1291\n","epoch:164 train_loss:7.6184 val_loss:14.7615\n","epoch:165 train_loss:7.0591 val_loss:14.8144\n","epoch:166 train_loss:7.3444 val_loss:14.6215\n","epoch:167 train_loss:6.8906 val_loss:14.6655\n","epoch:168 train_loss:6.9406 val_loss:14.5907\n","epoch:169 train_loss:6.0785 val_loss:14.2742\n","epoch:170 train_loss:6.4681 val_loss:14.2478\n","epoch:171 train_loss:5.8143 val_loss:14.1353\n","epoch:172 train_loss:5.9826 val_loss:14.2322\n","epoch:173 train_loss:5.6163 val_loss:14.3427\n","epoch:174 train_loss:5.5883 val_loss:14.4830\n","epoch:175 train_loss:6.1495 val_loss:14.3980\n","epoch:176 train_loss:5.2520 val_loss:14.3252\n","epoch:177 train_loss:5.3036 val_loss:14.3492\n","epoch:178 train_loss:5.1732 val_loss:14.4401\n","epoch:179 train_loss:5.6616 val_loss:14.3848\n","epoch:180 train_loss:4.9119 val_loss:14.2581\n","epoch:181 train_loss:4.5741 val_loss:14.2752\n","epoch:182 train_loss:4.5419 val_loss:14.4677\n","epoch:183 train_loss:5.1900 val_loss:14.3122\n","epoch:184 train_loss:4.4589 val_loss:14.4239\n","epoch:185 train_loss:4.1958 val_loss:14.3456\n","epoch:186 train_loss:4.4699 val_loss:14.2479\n","epoch:187 train_loss:4.3131 val_loss:14.4402\n","epoch:188 train_loss:4.1874 val_loss:14.1251\n","epoch:189 train_loss:3.9798 val_loss:14.2439\n","epoch:190 train_loss:4.0144 val_loss:14.2503\n","epoch:191 train_loss:3.9681 val_loss:14.2445\n","epoch:192 train_loss:4.2291 val_loss:14.2568\n","epoch:193 train_loss:3.8749 val_loss:14.4381\n","epoch:194 train_loss:3.9886 val_loss:14.3291\n","epoch:195 train_loss:3.2877 val_loss:14.3394\n","epoch:196 train_loss:3.4967 val_loss:14.1806\n","epoch:197 train_loss:3.7168 val_loss:14.5195\n","epoch:198 train_loss:3.4737 val_loss:14.1518\n","epoch:199 train_loss:3.4948 val_loss:14.1051\n","epoch:200 train_loss:3.2645 val_loss:14.4526\n"],"name":"stdout"}]},{"metadata":{"id":"aFdaJnp9BMDi","colab_type":"text"},"cell_type":"markdown","source":["## 音素認識結果の観察"]},{"metadata":{"id":"m01iLiSXBMDj","colab_type":"code","colab":{}},"cell_type":"code","source":["test_utterance_number = 0\n","\n","if gpu_id >= 0:\n","    net.to_gpu(gpu_id)\n","\n","Xs, _, __ = converter(test, gpu_id)\n","\n","# テストデータを1つ取り出します\n","X_test = Xs[test_utterance_number]\n","_, __, lab_test = test[test_utterance_number]\n","\n","with chainer.using_config('train', False), \\\n","     chainer.using_config('enable_backprop', False):\n","    y_test = net([X_test])\n","\n","# Variable形式で出てくるので中身を取り出す\n","y_test = y_test.array\n","\n","# 結果をCPUに送る\n","y_test = to_cpu(y_test)\n","\n","# 予測確率の最大値のインデックスを見る\n","pred_label = y_test.argmax(axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"80hRfkgVBMDl","colab_type":"text"},"cell_type":"markdown","source":["テストデータの0番目 (Jセット第1文) の正解音素列は次の通りです。"]},{"metadata":{"id":"CAkmVn0bBMDo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"2632f4db-b943-4caa-f9dc-abbc64245142","executionInfo":{"status":"ok","timestamp":1534999713799,"user_tz":-540,"elapsed":840,"user":{"displayName":"Hiroki Mori","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109920332067293067651"}}},"cell_type":"code","source":["le.inverse_transform(lab_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n","  if diff:\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array(['sil', 'ch', 'i', 'i', 's', 'a', 'n', 'a', 'u', 'n', 'a', 'g', 'i',\n","       'y', 'a', 'n', 'i', 'pau', 'n', 'e', 'Q', 'k', 'i', 'n', 'o', 'y',\n","       'o', 'o', 'n', 'a', 'm', 'o', 'n', 'o', 'g', 'a', 'm', 'i', 'n',\n","       'a', 'g', 'i', 'r', 'u', 'sil'], dtype='<U3')"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"cqadLsmbBMDr","colab_type":"text"},"cell_type":"markdown","source":["これに対し、まず各フレームに対する音素認識結果を見てみます。"]},{"metadata":{"id":"NJwoB9UpBMDt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":581},"outputId":"ce594edb-48d9-4e01-ab12-7e782a708aeb","executionInfo":{"status":"ok","timestamp":1534999715062,"user_tz":-540,"elapsed":1210,"user":{"displayName":"Hiroki Mori","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109920332067293067651"}}},"cell_type":"code","source":["le.inverse_transform(pred_label)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n","  if diff:\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array(['sil', 'sil', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n","       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n","       '_', '_', '_', '_', 'ch', 'ch', 'ch', '_', '_', 'i', 'i', '_', '_',\n","       '_', 'i', 'i', 'i', 'i', 'i', '_', '_', '_', '_', '_', '_', 's',\n","       's', '_', '_', '_', '_', 'a', 'a', '_', '_', '_', '_', 'n', 'n',\n","       'n', 'a', 'a', '_', '_', '_', '_', '_', '_', '_', '_', 'u', 'u',\n","       '_', '_', '_', '_', '_', '_', '_', '_', '_', 'n', 'n', 'a', 'a',\n","       'a', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'g', 'g',\n","       'g', '_', '_', '_', '_', '_', '_', 'i', 'i', 'i', 'i', '_', '_',\n","       '_', 'a', 'a', 'a', 'a', '_', '_', '_', '_', '_', '_', '_', '_',\n","       'n', 'n', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'i', 'i',\n","       'i', 'i', '_', '_', '_', '_', 'pau', 'pau', '_', '_', '_', '_',\n","       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n","       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n","       '_', '_', '_', '_', '_', 'n', '_', '_', '_', 'e', 'e', 'e', '_',\n","       '_', '_', '_', '_', '_', '_', '_', 'Q', 'Q', '_', '_', '_', '_',\n","       '_', 'k', 'k', '_', '_', '_', 'i', '_', '_', '_', '_', '_', 'n',\n","       'n', '_', '_', '_', 'o', 'o', 'o', '_', '_', '_', '_', '_', 'y',\n","       'y', '_', '_', 'o', 'o', 'o', '_', '_', '_', 'o', 'o', 'o', 'o',\n","       '_', '_', '_', '_', 'n', 'n', 'n', 'a', 'a', '_', '_', '_', '_',\n","       '_', '_', '_', '_', 'm', 'm', 'm', '_', '_', 'o', 'o', '_', '_',\n","       '_', '_', 'n', 'n', 'n', 'o', 'o', '_', '_', '_', '_', '_', '_',\n","       '_', 'g', 'a', 'a', 'a', 'a', '_', '_', '_', '_', '_', '_', '_',\n","       '_', '_', '_', '_', '_', 'm', 'm', '_', '_', '_', 'i', 'i', 'i',\n","       '_', '_', '_', '_', 'n', 'a', 'a', 'a', 'a', '_', '_', '_', '_',\n","       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'g', '_', '_',\n","       '_', '_', '_', '_', 'i', 'i', '_', '_', '_', 'r', 'r', 'r', 'u',\n","       'u', 'u', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n","       '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_',\n","       '_', '_', '_', '_', 'sil', 'sil'], dtype='<U3')"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"-rItLo8VBMDx","colab_type":"text"},"cell_type":"markdown","source":["多くのフレームで _ (ブランク) が出力されているのがわかります。これは、最も「自信がある」フレームに対してだけ具体的な音素が出力され、音素境界に近い曖昧な部分では何も出力していないものと解釈することができます。\n","\n","この結果をLSTM等と単純に比較することはできません。CTCの学習には音素の時間情報を使っていないので、こちらの方が不利だからです。\n","\n","出力された冗長な音素列を縮約することで、次のように最終的な音素認識結果が得られます。"]},{"metadata":{"id":"R-AljFiLBMDy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"f740e349-4cd2-4290-8d69-b64975d4d7e1","executionInfo":{"status":"ok","timestamp":1534999716374,"user_tz":-540,"elapsed":1253,"user":{"displayName":"Hiroki Mori","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"109920332067293067651"}}},"cell_type":"code","source":["mask = pred_label[:-1] != pred_label[1:]\n","tmp_label = pred_label[np.append(mask,True)]\n","mask = tmp_label != blank_symbol\n","le.inverse_transform(tmp_label[mask])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n","  if diff:\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array(['sil', 'ch', 'i', 'i', 's', 'a', 'n', 'a', 'u', 'n', 'a', 'g', 'i',\n","       'a', 'n', 'i', 'pau', 'n', 'e', 'Q', 'k', 'i', 'n', 'o', 'y', 'o',\n","       'o', 'n', 'a', 'm', 'o', 'n', 'o', 'g', 'a', 'm', 'i', 'n', 'a',\n","       'g', 'i', 'r', 'u', 'sil'], dtype='<U3')"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"L7b2g8VbBMD3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}