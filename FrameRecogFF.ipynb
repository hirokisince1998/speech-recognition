{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Qc-nixf70RC"
   },
   "source": [
    "# Feed-forwardネットワークを用いたフレームごとの音素認識\n",
    "\n",
    "Feed-forwardネットワークは、入力層-隠れ層-出力層の順にデータが流れる、もっとも基本的なニューラルネットです。多層パーセプトロン(MLP; Multy-Layer Perceptron)とも呼ばれます。\n",
    "\n",
    "ここではfeed-forwardネットワークを用いてフレームごとに音素の識別を行います。入力層のユニット数は特徴ベクトルの次元数(=26)。出力層のユニット数は音素の数(=41)で、出力の値は各音素である確率になるように学習します(後述)。\n",
    "\n",
    "隠れ層のユニット数`n_mid_units`は、ニューラルネットが表現できる識別境界の複雑さを制御する非常に重要な条件です。小さすぎると近似が粗くなりすぎ、大きすぎると訓練データに特化しすぎてテストデータに対する性能が落ちます。これは後で実験により調整します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1Yt0jhnz2IY"
   },
   "source": [
    "\n",
    "## ネットワーク構造の記述\n",
    "\n",
    "Feed-forwardネットワークでは、ネットワークの出力は、入力に対する隠れ層の出力を次の隠れ層に入力し、その出力を次の隠れ層に…と繰り返して計算します。\n",
    "\n",
    "`l1`は入力から1番目の隠れ層への結合を表しており、実体は $F$ 次元ベクトルから $H$ (=`n_mid_units`) 次元ベクトルへの線形変換(正確にいうとアフィン変換)ですから、  $F\\times H$ 行列です。\n",
    "\n",
    "`l2`は1番目の隠れ層から2番目の隠れ層への結合を表す $H\\times H$ 行列です。\n",
    "\n",
    "`l3`は2番目の隠れ層から出力層への結合を表す $H\\times D$ 行列です。($D$ は音素の数=41)\n",
    "\n",
    "隠れ層の出力は、線形変換の後に活性化関数に通すことで計算できます。今回はReLU関数を使います。入力`x`を`l1`で変換してReLUに通して1番目の隠れ層の出力`h1`を計算し、次に`h1`を`l2`で変換してReLUに通して2番目の出力`h2`を計算し、最後に`h2`を`l3`に通して出力を計算します。(最後の出力をさらに変換することもあるのですが、ここでは省略しています)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-evhguLG70RD"
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_mid_units=100, n_out=41):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # パラメータを持つ層の登録\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_mid_units)\n",
    "            self.l2 = L.Linear(n_mid_units, n_mid_units)\n",
    "            self.l3 = L.Linear(n_mid_units, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # データを受け取った際のforward計算を書く\n",
    "        x1 = F.concat(x, axis=0)\n",
    "        h1 = F.relu(self.l1(x1))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4MPO0mj70RL"
   },
   "source": [
    "`converter`では、上記`MLP`で読み込めるよう、データを行列のリストに変換しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Pyli4xYy70RM"
   },
   "outputs": [],
   "source": [
    "from chainer.dataset import to_device\n",
    "from chainer.dataset import concat_examples\n",
    "import numpy as np\n",
    "\n",
    "def converter(batch, device):\n",
    "    # alternative to chainer.dataset.concat_examples\n",
    "    \n",
    "    Xs = [to_device(device, X) for X, _, __ in batch]\n",
    "    ts = [to_device(device, t) for _, t, __ in batch]\n",
    "\n",
    "    lab_batch = [lab.astype(np.int32) for _, __, lab in batch]\n",
    "    labs = concat_examples(lab_batch, device, padding=0)\n",
    "    \n",
    "    return Xs, ts, labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YrgS_Hz70RP"
   },
   "source": [
    "## 出力の計算と損失関数\n",
    "\n",
    "音素のone-hot表現は、音素の数(=41)と同じ次元を持つ、要素1つだけが1、ほかが0の値を持つベクトルです。例えば音素 /a/ の番号は6なので、(先頭を0番目として)6番目だけが1、それ以外は0です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1535004278342,
     "user": {
      "displayName": "Tomhiro Nagata",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116093945812520810263"
     },
     "user_tz": -540
    },
    "id": "zl7TNxpg70RQ",
    "outputId": "93326240-3423-42f8-e637-af5f710ee9dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "with open('phones') as f:\n",
    "    phones = f.read().splitlines()\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform(le.fit_transform(phones).reshape(-1,1))\n",
    "ohe.transform([le.transform(['a'])]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJUWv4Fx70RU"
   },
   "source": [
    "`calculate_loss` 関数を定義します。この関数は、入力をネットワークに入れた出力を計算し、出力と正解ラベルから損失を計算します。\n",
    "\n",
    "バッチから`converter`で変換したデータ`Xs` (特徴ベクトル系列`X`の発話数分のリスト)を上で定義したネットワークに入れると出力`ys`が得られます。`ys`の各行`y`は、そのフレームに対する音素認識の結果を表します。\n",
    "\n",
    "一方、`ts`は各フレームの正解音素番号からなるベクトルです。\n",
    "`y`が音素のone-hot表現 $t_\\text{onehot}$ と一致するのが理想ですから、`y`が $t_\\text{onehot}$ と似ていないほど大きくなるような「損失関数」を定義し、計算される損失が小さくなるようにネットワークのパラメータ(ここでは`l1`, `l2`, `l3`)を調整します。分類問題では一般に交差エントロピーが損失関数として使われます。これを計算するのが `softmax_cross_entropy` 関数です。\n",
    "\n",
    "ついでに `accuracy` 関数で正解率も計算しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mKr9BpQW70RW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer.backends import cuda\n",
    "\n",
    "def calculate_loss(net, batch, gpu_id=0, train_mode=True):\n",
    "    numframes = [X.shape[0] for (X, t, lab) in batch]\n",
    "    \n",
    "    Xs, ts, _ = converter(batch, gpu_id) # 生ラベルは使わない\n",
    "\n",
    "    with chainer.using_config('train', train_mode), \\\n",
    "         chainer.using_config('enable_backprop', train_mode):\n",
    "        ys = net(Xs)\n",
    "    \n",
    "    ts = F.concat(ts, axis=0)\n",
    "    ts = ts.reshape(ts.shape[0],)\n",
    "    \n",
    "    # ロスを計算\n",
    "    loss = F.softmax_cross_entropy(ys, ts)\n",
    "\n",
    "    # 精度を計算\n",
    "    accuracy = F.accuracy(ys, ts)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJAXwXbC70RY"
   },
   "source": [
    "乱数を初期化する関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "f7a4SeaV70Ra"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import chainer\n",
    "\n",
    "def reset_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if gpu_id >= 0:\n",
    "        chainer.cuda.cupy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBlrHJpb70Rd"
   },
   "source": [
    "`phones` というファイルから音素リストを読み込んでおきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Yk2HOH8e70Re"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "with open('phones') as f:\n",
    "    phones = f.read().splitlines()\n",
    "le = LabelEncoder()\n",
    "le.fit(phones)\n",
    "nsymbol = len(phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcBe4l2a70Rh"
   },
   "source": [
    "## ネットワークの学習\n",
    "\n",
    "以下がネットワークを学習するプログラムです。\n",
    "\n",
    "設定可能なパラメータを上の方にまとめておきました。\n",
    "\n",
    "1. `gpu_id`:\n",
    "0にするとGPUで実行します。GPUがない場合は-1とします。\n",
    "1. `batchsize`:\n",
    "確率的勾配降下法(SGD)などを用いて学習する場合には、データをいくつかの固まり(ミニバッチ)に分割し、それぞれのミニバッチで1回のパラメータ更新を実行します。`batchsize`はミニバッチの大きさで、GPUに乗る限り大きくした方が計算が速くなります。\n",
    "1. `max_epoch`:\n",
    "学習データ全体にわたってミニバッチ学習を1回終えることを1エポックといいます。`max_epoch`は学習回数で、何エポック学習するかを設定します。\n",
    "1. `n_mid_units`:\n",
    "中間層ユニット数です。\n",
    "\n",
    "乱数の種をセットするのは、毎回同じ結果が出るようにするためです。初期値がまずいと学習がうまく進まないことがあります。おかしいと思った場合は種を変えてみてください。\n",
    "\n",
    "学習に使う最適化器(optimizer)には確率的勾配降下法を指定し、学習率は0.01にしています。\n",
    "\n",
    "whileループが学習のループです。ニューラルネットの学習で使われる勾配降下法は繰り返しアルゴリズムですので、何度も学習を繰り返してパラメータを調整します。\n",
    "\n",
    "ミニバッチに対して損失 `loss` を計算します。\n",
    "\n",
    "ニューラルネットの学習では、一般に誤差逆伝播アルゴリズムが使われます。`loss` に対して `backward()` メソッドを実行すると、誤差逆伝播アルゴリズムによりパラメータ更新に必要な誤差が計算されます。次に `optimizer.update()` メソッドを実行すると、計算された誤差をもとにパラメータが更新されます。これでミニバッチ1回の学習が終わりです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6817
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 136090,
     "status": "ok",
     "timestamp": 1535004453158,
     "user": {
      "displayName": "Tomhiro Nagata",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116093945812520810263"
     },
     "user_tz": -540
    },
    "id": "yJEzAaMg70Ri",
    "outputId": "ea54f685-7261-4fc9-9c4d-9e53249114fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:01 train_loss:2.9459 val_loss:2.8010 val_accuracy:0.3503\n",
      "epoch:02 train_loss:2.4236 val_loss:2.4030 val_accuracy:0.4097\n",
      "epoch:03 train_loss:2.1578 val_loss:2.1621 val_accuracy:0.4355\n",
      "epoch:04 train_loss:2.0519 val_loss:2.0399 val_accuracy:0.4564\n",
      "epoch:05 train_loss:1.9509 val_loss:1.9366 val_accuracy:0.4724\n",
      "epoch:06 train_loss:1.8821 val_loss:1.8804 val_accuracy:0.4830\n",
      "epoch:07 train_loss:1.7901 val_loss:1.8101 val_accuracy:0.4922\n",
      "epoch:08 train_loss:1.7337 val_loss:1.7659 val_accuracy:0.4995\n",
      "epoch:09 train_loss:1.6996 val_loss:1.7261 val_accuracy:0.5035\n",
      "epoch:10 train_loss:1.6756 val_loss:1.6889 val_accuracy:0.5105\n",
      "epoch:11 train_loss:1.6557 val_loss:1.6530 val_accuracy:0.5153\n",
      "epoch:12 train_loss:1.5956 val_loss:1.6309 val_accuracy:0.5176\n",
      "epoch:13 train_loss:1.5997 val_loss:1.6006 val_accuracy:0.5225\n",
      "epoch:14 train_loss:1.5612 val_loss:1.5884 val_accuracy:0.5245\n",
      "epoch:15 train_loss:1.5385 val_loss:1.5665 val_accuracy:0.5276\n",
      "epoch:16 train_loss:1.5525 val_loss:1.5442 val_accuracy:0.5308\n",
      "epoch:17 train_loss:1.4990 val_loss:1.5289 val_accuracy:0.5342\n",
      "epoch:18 train_loss:1.5019 val_loss:1.5143 val_accuracy:0.5360\n",
      "epoch:19 train_loss:1.4965 val_loss:1.4982 val_accuracy:0.5395\n",
      "epoch:20 train_loss:1.4511 val_loss:1.4875 val_accuracy:0.5407\n",
      "epoch:21 train_loss:1.4507 val_loss:1.4711 val_accuracy:0.5425\n",
      "epoch:22 train_loss:1.4422 val_loss:1.4634 val_accuracy:0.5449\n",
      "epoch:23 train_loss:1.4314 val_loss:1.4496 val_accuracy:0.5471\n",
      "epoch:24 train_loss:1.4114 val_loss:1.4435 val_accuracy:0.5473\n",
      "epoch:25 train_loss:1.4018 val_loss:1.4251 val_accuracy:0.5511\n",
      "epoch:26 train_loss:1.3913 val_loss:1.4248 val_accuracy:0.5518\n",
      "epoch:27 train_loss:1.3895 val_loss:1.4109 val_accuracy:0.5543\n",
      "epoch:28 train_loss:1.3707 val_loss:1.4050 val_accuracy:0.5556\n",
      "epoch:29 train_loss:1.3899 val_loss:1.3943 val_accuracy:0.5578\n",
      "epoch:30 train_loss:1.3782 val_loss:1.3936 val_accuracy:0.5562\n",
      "epoch:31 train_loss:1.3396 val_loss:1.3766 val_accuracy:0.5623\n",
      "epoch:32 train_loss:1.3473 val_loss:1.3735 val_accuracy:0.5621\n",
      "epoch:33 train_loss:1.3058 val_loss:1.3721 val_accuracy:0.5597\n",
      "epoch:34 train_loss:1.3525 val_loss:1.3594 val_accuracy:0.5648\n",
      "epoch:35 train_loss:1.3092 val_loss:1.3535 val_accuracy:0.5651\n",
      "epoch:36 train_loss:1.3382 val_loss:1.3469 val_accuracy:0.5682\n",
      "epoch:37 train_loss:1.3316 val_loss:1.3398 val_accuracy:0.5679\n",
      "epoch:38 train_loss:1.3135 val_loss:1.3402 val_accuracy:0.5662\n",
      "epoch:39 train_loss:1.2952 val_loss:1.3294 val_accuracy:0.5705\n",
      "epoch:40 train_loss:1.3164 val_loss:1.3281 val_accuracy:0.5701\n",
      "epoch:41 train_loss:1.2930 val_loss:1.3259 val_accuracy:0.5692\n",
      "epoch:42 train_loss:1.2964 val_loss:1.3204 val_accuracy:0.5711\n",
      "epoch:43 train_loss:1.3055 val_loss:1.3120 val_accuracy:0.5724\n",
      "epoch:44 train_loss:1.2920 val_loss:1.3171 val_accuracy:0.5691\n",
      "epoch:45 train_loss:1.2860 val_loss:1.3024 val_accuracy:0.5741\n",
      "epoch:46 train_loss:1.2933 val_loss:1.3008 val_accuracy:0.5746\n",
      "epoch:47 train_loss:1.2618 val_loss:1.2943 val_accuracy:0.5759\n",
      "epoch:48 train_loss:1.2631 val_loss:1.2960 val_accuracy:0.5737\n",
      "epoch:49 train_loss:1.2527 val_loss:1.2975 val_accuracy:0.5732\n",
      "epoch:50 train_loss:1.2614 val_loss:1.2838 val_accuracy:0.5777\n",
      "epoch:51 train_loss:1.2285 val_loss:1.2854 val_accuracy:0.5758\n",
      "epoch:52 train_loss:1.2622 val_loss:1.2758 val_accuracy:0.5792\n",
      "epoch:53 train_loss:1.2430 val_loss:1.2726 val_accuracy:0.5792\n",
      "epoch:54 train_loss:1.2260 val_loss:1.2684 val_accuracy:0.5814\n",
      "epoch:55 train_loss:1.2558 val_loss:1.2641 val_accuracy:0.5808\n",
      "epoch:56 train_loss:1.2115 val_loss:1.2633 val_accuracy:0.5820\n",
      "epoch:57 train_loss:1.2446 val_loss:1.2636 val_accuracy:0.5819\n",
      "epoch:58 train_loss:1.2194 val_loss:1.2577 val_accuracy:0.5820\n",
      "epoch:59 train_loss:1.2401 val_loss:1.2480 val_accuracy:0.5866\n",
      "epoch:60 train_loss:1.2234 val_loss:1.2485 val_accuracy:0.5854\n",
      "epoch:61 train_loss:1.2204 val_loss:1.2494 val_accuracy:0.5839\n",
      "epoch:62 train_loss:1.2383 val_loss:1.2419 val_accuracy:0.5867\n",
      "epoch:63 train_loss:1.2164 val_loss:1.2490 val_accuracy:0.5834\n",
      "epoch:64 train_loss:1.1809 val_loss:1.2396 val_accuracy:0.5872\n",
      "epoch:65 train_loss:1.1876 val_loss:1.2382 val_accuracy:0.5861\n",
      "epoch:66 train_loss:1.2149 val_loss:1.2345 val_accuracy:0.5868\n",
      "epoch:67 train_loss:1.1902 val_loss:1.2359 val_accuracy:0.5859\n",
      "epoch:68 train_loss:1.2135 val_loss:1.2298 val_accuracy:0.5874\n",
      "epoch:69 train_loss:1.1790 val_loss:1.2282 val_accuracy:0.5888\n",
      "epoch:70 train_loss:1.1990 val_loss:1.2210 val_accuracy:0.5903\n",
      "epoch:71 train_loss:1.1862 val_loss:1.2268 val_accuracy:0.5876\n",
      "epoch:72 train_loss:1.1864 val_loss:1.2266 val_accuracy:0.5888\n",
      "epoch:73 train_loss:1.1918 val_loss:1.2145 val_accuracy:0.5910\n",
      "epoch:74 train_loss:1.1993 val_loss:1.2157 val_accuracy:0.5912\n",
      "epoch:75 train_loss:1.2066 val_loss:1.2116 val_accuracy:0.5923\n",
      "epoch:76 train_loss:1.1705 val_loss:1.2099 val_accuracy:0.5927\n",
      "epoch:77 train_loss:1.2014 val_loss:1.2072 val_accuracy:0.5937\n",
      "epoch:78 train_loss:1.1731 val_loss:1.2087 val_accuracy:0.5934\n",
      "epoch:79 train_loss:1.1757 val_loss:1.2029 val_accuracy:0.5950\n",
      "epoch:80 train_loss:1.1659 val_loss:1.2081 val_accuracy:0.5921\n",
      "epoch:81 train_loss:1.1679 val_loss:1.2023 val_accuracy:0.5962\n",
      "epoch:82 train_loss:1.1722 val_loss:1.2012 val_accuracy:0.5949\n",
      "epoch:83 train_loss:1.1924 val_loss:1.1977 val_accuracy:0.5954\n",
      "epoch:84 train_loss:1.1630 val_loss:1.1914 val_accuracy:0.5970\n",
      "epoch:85 train_loss:1.1739 val_loss:1.1992 val_accuracy:0.5934\n",
      "epoch:86 train_loss:1.1444 val_loss:1.1987 val_accuracy:0.5944\n",
      "epoch:87 train_loss:1.1410 val_loss:1.1950 val_accuracy:0.5947\n",
      "epoch:88 train_loss:1.1531 val_loss:1.1935 val_accuracy:0.5950\n",
      "epoch:89 train_loss:1.1547 val_loss:1.1884 val_accuracy:0.5973\n",
      "epoch:90 train_loss:1.1393 val_loss:1.1848 val_accuracy:0.5987\n",
      "epoch:91 train_loss:1.1519 val_loss:1.1851 val_accuracy:0.5988\n",
      "epoch:92 train_loss:1.1479 val_loss:1.1820 val_accuracy:0.5993\n",
      "epoch:93 train_loss:1.1217 val_loss:1.1786 val_accuracy:0.6008\n",
      "epoch:94 train_loss:1.1425 val_loss:1.1841 val_accuracy:0.5963\n",
      "epoch:95 train_loss:1.1373 val_loss:1.1812 val_accuracy:0.5977\n",
      "epoch:96 train_loss:1.1370 val_loss:1.1746 val_accuracy:0.6017\n",
      "epoch:97 train_loss:1.1364 val_loss:1.1682 val_accuracy:0.6029\n",
      "epoch:98 train_loss:1.1438 val_loss:1.1713 val_accuracy:0.6023\n",
      "epoch:99 train_loss:1.1331 val_loss:1.1724 val_accuracy:0.6003\n",
      "epoch:100 train_loss:1.1168 val_loss:1.1696 val_accuracy:0.6029\n",
      "epoch:101 train_loss:1.1334 val_loss:1.1689 val_accuracy:0.6022\n",
      "epoch:102 train_loss:1.1287 val_loss:1.1685 val_accuracy:0.6018\n",
      "epoch:103 train_loss:1.1117 val_loss:1.1620 val_accuracy:0.6044\n",
      "epoch:104 train_loss:1.1265 val_loss:1.1677 val_accuracy:0.6000\n",
      "epoch:105 train_loss:1.0918 val_loss:1.1590 val_accuracy:0.6048\n",
      "epoch:106 train_loss:1.1283 val_loss:1.1591 val_accuracy:0.6053\n",
      "epoch:107 train_loss:1.1292 val_loss:1.1583 val_accuracy:0.6058\n",
      "epoch:108 train_loss:1.1254 val_loss:1.1634 val_accuracy:0.6018\n",
      "epoch:109 train_loss:1.1474 val_loss:1.1593 val_accuracy:0.6040\n",
      "epoch:110 train_loss:1.1046 val_loss:1.1600 val_accuracy:0.6033\n",
      "epoch:111 train_loss:1.1176 val_loss:1.1546 val_accuracy:0.6058\n",
      "epoch:112 train_loss:1.1110 val_loss:1.1559 val_accuracy:0.6031\n",
      "epoch:113 train_loss:1.1267 val_loss:1.1494 val_accuracy:0.6066\n",
      "epoch:114 train_loss:1.1125 val_loss:1.1506 val_accuracy:0.6062\n",
      "epoch:115 train_loss:1.1156 val_loss:1.1474 val_accuracy:0.6078\n",
      "epoch:116 train_loss:1.1214 val_loss:1.1496 val_accuracy:0.6075\n",
      "epoch:117 train_loss:1.1136 val_loss:1.1511 val_accuracy:0.6048\n",
      "epoch:118 train_loss:1.1193 val_loss:1.1463 val_accuracy:0.6067\n",
      "epoch:119 train_loss:1.0958 val_loss:1.1499 val_accuracy:0.6036\n",
      "epoch:120 train_loss:1.1093 val_loss:1.1440 val_accuracy:0.6070\n",
      "epoch:121 train_loss:1.0996 val_loss:1.1405 val_accuracy:0.6075\n",
      "epoch:122 train_loss:1.1002 val_loss:1.1370 val_accuracy:0.6102\n",
      "epoch:123 train_loss:1.0941 val_loss:1.1465 val_accuracy:0.6052\n",
      "epoch:124 train_loss:1.0869 val_loss:1.1426 val_accuracy:0.6060\n",
      "epoch:125 train_loss:1.0824 val_loss:1.1371 val_accuracy:0.6093\n",
      "epoch:126 train_loss:1.0981 val_loss:1.1322 val_accuracy:0.6111\n",
      "epoch:127 train_loss:1.1030 val_loss:1.1445 val_accuracy:0.6046\n",
      "epoch:128 train_loss:1.0946 val_loss:1.1322 val_accuracy:0.6107\n",
      "epoch:129 train_loss:1.0849 val_loss:1.1308 val_accuracy:0.6105\n",
      "epoch:130 train_loss:1.1007 val_loss:1.1260 val_accuracy:0.6131\n",
      "epoch:131 train_loss:1.0971 val_loss:1.1252 val_accuracy:0.6130\n",
      "epoch:132 train_loss:1.0861 val_loss:1.1283 val_accuracy:0.6120\n",
      "epoch:133 train_loss:1.0714 val_loss:1.1241 val_accuracy:0.6136\n",
      "epoch:134 train_loss:1.1057 val_loss:1.1238 val_accuracy:0.6134\n",
      "epoch:135 train_loss:1.0850 val_loss:1.1239 val_accuracy:0.6141\n",
      "epoch:136 train_loss:1.0847 val_loss:1.1199 val_accuracy:0.6145\n",
      "epoch:137 train_loss:1.0978 val_loss:1.1318 val_accuracy:0.6084\n",
      "epoch:138 train_loss:1.0649 val_loss:1.1205 val_accuracy:0.6141\n",
      "epoch:139 train_loss:1.0734 val_loss:1.1236 val_accuracy:0.6101\n",
      "epoch:140 train_loss:1.0826 val_loss:1.1197 val_accuracy:0.6137\n",
      "epoch:141 train_loss:1.0719 val_loss:1.1162 val_accuracy:0.6161\n",
      "epoch:142 train_loss:1.0887 val_loss:1.1176 val_accuracy:0.6129\n",
      "epoch:143 train_loss:1.0783 val_loss:1.1190 val_accuracy:0.6131\n",
      "epoch:144 train_loss:1.0655 val_loss:1.1188 val_accuracy:0.6126\n",
      "epoch:145 train_loss:1.0720 val_loss:1.1120 val_accuracy:0.6176\n",
      "epoch:146 train_loss:1.0827 val_loss:1.1151 val_accuracy:0.6149\n",
      "epoch:147 train_loss:1.0575 val_loss:1.1105 val_accuracy:0.6173\n",
      "epoch:148 train_loss:1.0734 val_loss:1.1084 val_accuracy:0.6177\n",
      "epoch:149 train_loss:1.0819 val_loss:1.1149 val_accuracy:0.6153\n",
      "epoch:150 train_loss:1.0680 val_loss:1.1074 val_accuracy:0.6188\n",
      "epoch:151 train_loss:1.0544 val_loss:1.1013 val_accuracy:0.6211\n",
      "epoch:152 train_loss:1.0637 val_loss:1.1070 val_accuracy:0.6169\n",
      "epoch:153 train_loss:1.0680 val_loss:1.1068 val_accuracy:0.6155\n",
      "epoch:154 train_loss:1.0615 val_loss:1.1087 val_accuracy:0.6153\n",
      "epoch:155 train_loss:1.0716 val_loss:1.1067 val_accuracy:0.6167\n",
      "epoch:156 train_loss:1.0520 val_loss:1.1075 val_accuracy:0.6154\n",
      "epoch:157 train_loss:1.0632 val_loss:1.1062 val_accuracy:0.6156\n",
      "epoch:158 train_loss:1.0582 val_loss:1.1015 val_accuracy:0.6171\n",
      "epoch:159 train_loss:1.0434 val_loss:1.1035 val_accuracy:0.6158\n",
      "epoch:160 train_loss:1.0407 val_loss:1.1010 val_accuracy:0.6178\n",
      "epoch:161 train_loss:1.0659 val_loss:1.1087 val_accuracy:0.6144\n",
      "epoch:162 train_loss:1.0586 val_loss:1.0988 val_accuracy:0.6182\n",
      "epoch:163 train_loss:1.0559 val_loss:1.1028 val_accuracy:0.6166\n",
      "epoch:164 train_loss:1.0372 val_loss:1.1097 val_accuracy:0.6138\n",
      "epoch:165 train_loss:1.0734 val_loss:1.0955 val_accuracy:0.6203\n",
      "epoch:166 train_loss:1.0582 val_loss:1.0925 val_accuracy:0.6220\n",
      "epoch:167 train_loss:1.0596 val_loss:1.0909 val_accuracy:0.6220\n",
      "epoch:168 train_loss:1.0385 val_loss:1.0943 val_accuracy:0.6200\n",
      "epoch:169 train_loss:1.0488 val_loss:1.0873 val_accuracy:0.6233\n",
      "epoch:170 train_loss:1.0593 val_loss:1.0964 val_accuracy:0.6178\n",
      "epoch:171 train_loss:1.0738 val_loss:1.0881 val_accuracy:0.6240\n",
      "epoch:172 train_loss:1.0351 val_loss:1.0872 val_accuracy:0.6230\n",
      "epoch:173 train_loss:1.0425 val_loss:1.0951 val_accuracy:0.6193\n",
      "epoch:174 train_loss:1.0683 val_loss:1.0880 val_accuracy:0.6217\n",
      "epoch:175 train_loss:1.0451 val_loss:1.0908 val_accuracy:0.6198\n",
      "epoch:176 train_loss:1.0414 val_loss:1.0901 val_accuracy:0.6196\n",
      "epoch:177 train_loss:1.0628 val_loss:1.0865 val_accuracy:0.6233\n",
      "epoch:178 train_loss:1.0399 val_loss:1.0882 val_accuracy:0.6200\n",
      "epoch:179 train_loss:1.0407 val_loss:1.0850 val_accuracy:0.6218\n",
      "epoch:180 train_loss:1.0280 val_loss:1.0820 val_accuracy:0.6233\n",
      "epoch:181 train_loss:1.0275 val_loss:1.0827 val_accuracy:0.6231\n",
      "epoch:182 train_loss:1.0575 val_loss:1.0897 val_accuracy:0.6209\n",
      "epoch:183 train_loss:1.0392 val_loss:1.0797 val_accuracy:0.6240\n",
      "epoch:184 train_loss:1.0444 val_loss:1.0809 val_accuracy:0.6220\n",
      "epoch:185 train_loss:1.0326 val_loss:1.0838 val_accuracy:0.6200\n",
      "epoch:186 train_loss:1.0216 val_loss:1.0789 val_accuracy:0.6244\n",
      "epoch:187 train_loss:1.0349 val_loss:1.0753 val_accuracy:0.6264\n",
      "epoch:188 train_loss:1.0389 val_loss:1.0776 val_accuracy:0.6256\n",
      "epoch:189 train_loss:1.0193 val_loss:1.0768 val_accuracy:0.6238\n",
      "epoch:190 train_loss:1.0283 val_loss:1.0754 val_accuracy:0.6236\n",
      "epoch:191 train_loss:1.0368 val_loss:1.0742 val_accuracy:0.6268\n",
      "epoch:192 train_loss:1.0331 val_loss:1.0771 val_accuracy:0.6237\n",
      "epoch:193 train_loss:1.0150 val_loss:1.0707 val_accuracy:0.6264\n",
      "epoch:194 train_loss:1.0423 val_loss:1.0803 val_accuracy:0.6205\n",
      "epoch:195 train_loss:1.0606 val_loss:1.0714 val_accuracy:0.6258\n",
      "epoch:196 train_loss:1.0156 val_loss:1.0760 val_accuracy:0.6225\n",
      "epoch:197 train_loss:1.0356 val_loss:1.0673 val_accuracy:0.6288\n",
      "epoch:198 train_loss:1.0328 val_loss:1.0659 val_accuracy:0.6294\n",
      "epoch:199 train_loss:1.0392 val_loss:1.0763 val_accuracy:0.6209\n",
      "epoch:200 train_loss:1.0171 val_loss:1.0680 val_accuracy:0.6266\n",
      "epoch:201 train_loss:1.0352 val_loss:1.0711 val_accuracy:0.6241\n",
      "epoch:202 train_loss:1.0202 val_loss:1.0715 val_accuracy:0.6234\n",
      "epoch:203 train_loss:1.0502 val_loss:1.0669 val_accuracy:0.6277\n",
      "epoch:204 train_loss:1.0296 val_loss:1.0620 val_accuracy:0.6302\n",
      "epoch:205 train_loss:1.0192 val_loss:1.0743 val_accuracy:0.6217\n",
      "epoch:206 train_loss:1.0126 val_loss:1.0661 val_accuracy:0.6248\n",
      "epoch:207 train_loss:1.0374 val_loss:1.0600 val_accuracy:0.6294\n",
      "epoch:208 train_loss:1.0435 val_loss:1.0640 val_accuracy:0.6273\n",
      "epoch:209 train_loss:1.0149 val_loss:1.0679 val_accuracy:0.6239\n",
      "epoch:210 train_loss:1.0008 val_loss:1.0637 val_accuracy:0.6249\n",
      "epoch:211 train_loss:1.0248 val_loss:1.0672 val_accuracy:0.6228\n",
      "epoch:212 train_loss:1.0101 val_loss:1.0614 val_accuracy:0.6274\n",
      "epoch:213 train_loss:1.0164 val_loss:1.0588 val_accuracy:0.6297\n",
      "epoch:214 train_loss:1.0106 val_loss:1.0596 val_accuracy:0.6267\n",
      "epoch:215 train_loss:1.0267 val_loss:1.0610 val_accuracy:0.6292\n",
      "epoch:216 train_loss:1.0050 val_loss:1.0625 val_accuracy:0.6240\n",
      "epoch:217 train_loss:1.0033 val_loss:1.0669 val_accuracy:0.6226\n",
      "epoch:218 train_loss:1.0135 val_loss:1.0576 val_accuracy:0.6279\n",
      "epoch:219 train_loss:1.0122 val_loss:1.0579 val_accuracy:0.6290\n",
      "epoch:220 train_loss:1.0140 val_loss:1.0619 val_accuracy:0.6246\n",
      "epoch:221 train_loss:0.9735 val_loss:1.0574 val_accuracy:0.6272\n",
      "epoch:222 train_loss:0.9890 val_loss:1.0622 val_accuracy:0.6241\n",
      "epoch:223 train_loss:1.0108 val_loss:1.0520 val_accuracy:0.6315\n",
      "epoch:224 train_loss:1.0115 val_loss:1.0570 val_accuracy:0.6272\n",
      "epoch:225 train_loss:1.0302 val_loss:1.0560 val_accuracy:0.6284\n",
      "epoch:226 train_loss:1.0214 val_loss:1.0535 val_accuracy:0.6285\n",
      "epoch:227 train_loss:1.0170 val_loss:1.0520 val_accuracy:0.6298\n",
      "epoch:228 train_loss:1.0124 val_loss:1.0517 val_accuracy:0.6301\n",
      "epoch:229 train_loss:1.0292 val_loss:1.0582 val_accuracy:0.6256\n",
      "epoch:230 train_loss:1.0014 val_loss:1.0506 val_accuracy:0.6291\n",
      "epoch:231 train_loss:1.0163 val_loss:1.0476 val_accuracy:0.6308\n",
      "epoch:232 train_loss:0.9924 val_loss:1.0496 val_accuracy:0.6302\n",
      "epoch:233 train_loss:1.0005 val_loss:1.0503 val_accuracy:0.6278\n",
      "epoch:234 train_loss:1.0046 val_loss:1.0513 val_accuracy:0.6281\n",
      "epoch:235 train_loss:0.9873 val_loss:1.0476 val_accuracy:0.6299\n",
      "epoch:236 train_loss:1.0304 val_loss:1.0468 val_accuracy:0.6324\n",
      "epoch:237 train_loss:1.0148 val_loss:1.0601 val_accuracy:0.6224\n",
      "epoch:238 train_loss:0.9932 val_loss:1.0556 val_accuracy:0.6261\n",
      "epoch:239 train_loss:0.9918 val_loss:1.0480 val_accuracy:0.6290\n",
      "epoch:240 train_loss:0.9996 val_loss:1.0474 val_accuracy:0.6287\n",
      "epoch:241 train_loss:1.0000 val_loss:1.0430 val_accuracy:0.6314\n",
      "epoch:242 train_loss:0.9829 val_loss:1.0519 val_accuracy:0.6263\n",
      "epoch:243 train_loss:1.0036 val_loss:1.0411 val_accuracy:0.6341\n",
      "epoch:244 train_loss:1.0068 val_loss:1.0436 val_accuracy:0.6332\n",
      "epoch:245 train_loss:1.0010 val_loss:1.0481 val_accuracy:0.6285\n",
      "epoch:246 train_loss:0.9961 val_loss:1.0441 val_accuracy:0.6301\n",
      "epoch:247 train_loss:0.9951 val_loss:1.0403 val_accuracy:0.6322\n",
      "epoch:248 train_loss:1.0175 val_loss:1.0380 val_accuracy:0.6350\n",
      "epoch:249 train_loss:0.9784 val_loss:1.0377 val_accuracy:0.6324\n",
      "epoch:250 train_loss:0.9990 val_loss:1.0403 val_accuracy:0.6329\n",
      "epoch:251 train_loss:1.0029 val_loss:1.0586 val_accuracy:0.6250\n",
      "epoch:252 train_loss:1.0058 val_loss:1.0398 val_accuracy:0.6324\n",
      "epoch:253 train_loss:0.9920 val_loss:1.0392 val_accuracy:0.6307\n",
      "epoch:254 train_loss:1.0089 val_loss:1.0329 val_accuracy:0.6362\n",
      "epoch:255 train_loss:0.9974 val_loss:1.0415 val_accuracy:0.6288\n",
      "epoch:256 train_loss:0.9787 val_loss:1.0393 val_accuracy:0.6304\n",
      "epoch:257 train_loss:0.9864 val_loss:1.0340 val_accuracy:0.6359\n",
      "epoch:258 train_loss:0.9900 val_loss:1.0400 val_accuracy:0.6301\n",
      "epoch:259 train_loss:1.0015 val_loss:1.0326 val_accuracy:0.6373\n",
      "epoch:260 train_loss:0.9866 val_loss:1.0362 val_accuracy:0.6331\n",
      "epoch:261 train_loss:0.9842 val_loss:1.0367 val_accuracy:0.6310\n",
      "epoch:262 train_loss:0.9738 val_loss:1.0360 val_accuracy:0.6315\n",
      "epoch:263 train_loss:0.9722 val_loss:1.0312 val_accuracy:0.6344\n",
      "epoch:264 train_loss:0.9939 val_loss:1.0292 val_accuracy:0.6390\n",
      "epoch:265 train_loss:0.9858 val_loss:1.0328 val_accuracy:0.6341\n",
      "epoch:266 train_loss:1.0038 val_loss:1.0334 val_accuracy:0.6332\n",
      "epoch:267 train_loss:0.9733 val_loss:1.0377 val_accuracy:0.6295\n",
      "epoch:268 train_loss:1.0062 val_loss:1.0392 val_accuracy:0.6301\n",
      "epoch:269 train_loss:0.9892 val_loss:1.0356 val_accuracy:0.6309\n",
      "epoch:270 train_loss:0.9818 val_loss:1.0325 val_accuracy:0.6331\n",
      "epoch:271 train_loss:0.9990 val_loss:1.0333 val_accuracy:0.6328\n",
      "epoch:272 train_loss:0.9842 val_loss:1.0286 val_accuracy:0.6346\n",
      "epoch:273 train_loss:0.9862 val_loss:1.0331 val_accuracy:0.6334\n",
      "epoch:274 train_loss:1.0054 val_loss:1.0279 val_accuracy:0.6363\n",
      "epoch:275 train_loss:1.0004 val_loss:1.0231 val_accuracy:0.6388\n",
      "epoch:276 train_loss:0.9757 val_loss:1.0270 val_accuracy:0.6356\n",
      "epoch:277 train_loss:0.9747 val_loss:1.0409 val_accuracy:0.6289\n",
      "epoch:278 train_loss:0.9840 val_loss:1.0309 val_accuracy:0.6315\n",
      "epoch:279 train_loss:0.9798 val_loss:1.0243 val_accuracy:0.6365\n",
      "epoch:280 train_loss:0.9810 val_loss:1.0246 val_accuracy:0.6374\n",
      "epoch:281 train_loss:0.9735 val_loss:1.0249 val_accuracy:0.6352\n",
      "epoch:282 train_loss:0.9826 val_loss:1.0254 val_accuracy:0.6362\n",
      "epoch:283 train_loss:0.9862 val_loss:1.0281 val_accuracy:0.6326\n",
      "epoch:284 train_loss:0.9756 val_loss:1.0232 val_accuracy:0.6385\n",
      "epoch:285 train_loss:0.9793 val_loss:1.0237 val_accuracy:0.6354\n",
      "epoch:286 train_loss:0.9770 val_loss:1.0221 val_accuracy:0.6377\n",
      "epoch:287 train_loss:0.9789 val_loss:1.0232 val_accuracy:0.6365\n",
      "epoch:288 train_loss:0.9833 val_loss:1.0204 val_accuracy:0.6380\n",
      "epoch:289 train_loss:0.9822 val_loss:1.0285 val_accuracy:0.6323\n",
      "epoch:290 train_loss:0.9867 val_loss:1.0193 val_accuracy:0.6381\n",
      "epoch:291 train_loss:0.9648 val_loss:1.0223 val_accuracy:0.6367\n",
      "epoch:292 train_loss:0.9734 val_loss:1.0207 val_accuracy:0.6367\n",
      "epoch:293 train_loss:0.9646 val_loss:1.0215 val_accuracy:0.6348\n",
      "epoch:294 train_loss:0.9692 val_loss:1.0211 val_accuracy:0.6329\n",
      "epoch:295 train_loss:0.9610 val_loss:1.0301 val_accuracy:0.6304\n",
      "epoch:296 train_loss:0.9777 val_loss:1.0217 val_accuracy:0.6355\n",
      "epoch:297 train_loss:0.9724 val_loss:1.0158 val_accuracy:0.6388\n",
      "epoch:298 train_loss:0.9632 val_loss:1.0277 val_accuracy:0.6325\n",
      "epoch:299 train_loss:0.9666 val_loss:1.0143 val_accuracy:0.6385\n",
      "epoch:300 train_loss:0.9756 val_loss:1.0146 val_accuracy:0.6390\n",
      "epoch:301 train_loss:0.9618 val_loss:1.0170 val_accuracy:0.6393\n",
      "epoch:302 train_loss:0.9682 val_loss:1.0122 val_accuracy:0.6419\n",
      "epoch:303 train_loss:0.9615 val_loss:1.0181 val_accuracy:0.6361\n",
      "epoch:304 train_loss:0.9739 val_loss:1.0166 val_accuracy:0.6382\n",
      "epoch:305 train_loss:0.9534 val_loss:1.0129 val_accuracy:0.6391\n",
      "epoch:306 train_loss:0.9669 val_loss:1.0107 val_accuracy:0.6408\n",
      "epoch:307 train_loss:0.9748 val_loss:1.0110 val_accuracy:0.6417\n",
      "epoch:308 train_loss:0.9668 val_loss:1.0114 val_accuracy:0.6409\n",
      "epoch:309 train_loss:0.9651 val_loss:1.0262 val_accuracy:0.6324\n",
      "epoch:310 train_loss:0.9667 val_loss:1.0140 val_accuracy:0.6371\n",
      "epoch:311 train_loss:0.9558 val_loss:1.0170 val_accuracy:0.6344\n",
      "epoch:312 train_loss:0.9636 val_loss:1.0240 val_accuracy:0.6339\n",
      "epoch:313 train_loss:0.9607 val_loss:1.0127 val_accuracy:0.6381\n",
      "epoch:314 train_loss:0.9585 val_loss:1.0150 val_accuracy:0.6355\n",
      "epoch:315 train_loss:0.9786 val_loss:1.0193 val_accuracy:0.6334\n",
      "epoch:316 train_loss:0.9679 val_loss:1.0144 val_accuracy:0.6369\n",
      "epoch:317 train_loss:0.9601 val_loss:1.0092 val_accuracy:0.6395\n",
      "epoch:318 train_loss:0.9652 val_loss:1.0083 val_accuracy:0.6414\n",
      "epoch:319 train_loss:0.9734 val_loss:1.0212 val_accuracy:0.6333\n",
      "epoch:320 train_loss:0.9652 val_loss:1.0074 val_accuracy:0.6424\n",
      "epoch:321 train_loss:0.9400 val_loss:1.0102 val_accuracy:0.6375\n",
      "epoch:322 train_loss:0.9606 val_loss:1.0141 val_accuracy:0.6365\n",
      "epoch:323 train_loss:0.9619 val_loss:1.0047 val_accuracy:0.6430\n",
      "epoch:324 train_loss:0.9436 val_loss:1.0178 val_accuracy:0.6342\n",
      "epoch:325 train_loss:0.9668 val_loss:1.0097 val_accuracy:0.6383\n",
      "epoch:326 train_loss:0.9656 val_loss:1.0069 val_accuracy:0.6412\n",
      "epoch:327 train_loss:0.9583 val_loss:1.0160 val_accuracy:0.6349\n",
      "epoch:328 train_loss:0.9642 val_loss:1.0087 val_accuracy:0.6372\n",
      "epoch:329 train_loss:0.9569 val_loss:1.0056 val_accuracy:0.6390\n",
      "epoch:330 train_loss:0.9675 val_loss:1.0192 val_accuracy:0.6352\n",
      "epoch:331 train_loss:0.9795 val_loss:1.0022 val_accuracy:0.6431\n",
      "epoch:332 train_loss:0.9577 val_loss:1.0013 val_accuracy:0.6443\n",
      "epoch:333 train_loss:0.9477 val_loss:1.0115 val_accuracy:0.6366\n",
      "epoch:334 train_loss:0.9535 val_loss:1.0041 val_accuracy:0.6406\n",
      "epoch:335 train_loss:0.9580 val_loss:1.0143 val_accuracy:0.6355\n",
      "epoch:336 train_loss:0.9511 val_loss:1.0039 val_accuracy:0.6403\n",
      "epoch:337 train_loss:0.9433 val_loss:1.0053 val_accuracy:0.6386\n",
      "epoch:338 train_loss:0.9523 val_loss:1.0053 val_accuracy:0.6391\n",
      "epoch:339 train_loss:0.9430 val_loss:1.0129 val_accuracy:0.6360\n",
      "epoch:340 train_loss:0.9595 val_loss:1.0118 val_accuracy:0.6359\n",
      "epoch:341 train_loss:0.9518 val_loss:0.9996 val_accuracy:0.6432\n",
      "epoch:342 train_loss:0.9621 val_loss:0.9997 val_accuracy:0.6434\n",
      "epoch:343 train_loss:0.9371 val_loss:0.9972 val_accuracy:0.6431\n",
      "epoch:344 train_loss:0.9410 val_loss:1.0041 val_accuracy:0.6389\n",
      "epoch:345 train_loss:0.9580 val_loss:0.9972 val_accuracy:0.6443\n",
      "epoch:346 train_loss:0.9624 val_loss:0.9988 val_accuracy:0.6424\n",
      "epoch:347 train_loss:0.9558 val_loss:1.0062 val_accuracy:0.6376\n",
      "epoch:348 train_loss:0.9514 val_loss:1.0088 val_accuracy:0.6350\n",
      "epoch:349 train_loss:0.9548 val_loss:0.9985 val_accuracy:0.6432\n",
      "epoch:350 train_loss:0.9394 val_loss:1.0041 val_accuracy:0.6380\n",
      "epoch:351 train_loss:0.9612 val_loss:0.9956 val_accuracy:0.6433\n",
      "epoch:352 train_loss:0.9336 val_loss:0.9923 val_accuracy:0.6474\n",
      "epoch:353 train_loss:0.9464 val_loss:1.0220 val_accuracy:0.6339\n",
      "epoch:354 train_loss:0.9396 val_loss:1.0027 val_accuracy:0.6379\n",
      "epoch:355 train_loss:0.9503 val_loss:0.9953 val_accuracy:0.6430\n",
      "epoch:356 train_loss:0.9455 val_loss:0.9960 val_accuracy:0.6413\n",
      "epoch:357 train_loss:0.9504 val_loss:0.9907 val_accuracy:0.6468\n",
      "epoch:358 train_loss:0.9312 val_loss:0.9882 val_accuracy:0.6471\n",
      "epoch:359 train_loss:0.9346 val_loss:1.0038 val_accuracy:0.6377\n",
      "epoch:360 train_loss:0.9503 val_loss:1.0137 val_accuracy:0.6365\n",
      "epoch:361 train_loss:0.9480 val_loss:0.9904 val_accuracy:0.6466\n",
      "epoch:362 train_loss:0.9479 val_loss:0.9896 val_accuracy:0.6471\n",
      "epoch:363 train_loss:0.9490 val_loss:0.9893 val_accuracy:0.6463\n",
      "epoch:364 train_loss:0.9337 val_loss:0.9911 val_accuracy:0.6458\n",
      "epoch:365 train_loss:0.9484 val_loss:0.9863 val_accuracy:0.6481\n",
      "epoch:366 train_loss:0.9517 val_loss:0.9881 val_accuracy:0.6495\n",
      "epoch:367 train_loss:0.9298 val_loss:1.0028 val_accuracy:0.6379\n",
      "epoch:368 train_loss:0.9468 val_loss:0.9927 val_accuracy:0.6416\n",
      "epoch:369 train_loss:0.9303 val_loss:0.9883 val_accuracy:0.6455\n",
      "epoch:370 train_loss:0.9409 val_loss:0.9855 val_accuracy:0.6496\n",
      "epoch:371 train_loss:0.9285 val_loss:1.0032 val_accuracy:0.6376\n",
      "epoch:372 train_loss:0.9390 val_loss:1.0101 val_accuracy:0.6369\n",
      "epoch:373 train_loss:0.9287 val_loss:0.9891 val_accuracy:0.6469\n",
      "epoch:374 train_loss:0.9362 val_loss:0.9963 val_accuracy:0.6387\n",
      "epoch:375 train_loss:0.9328 val_loss:0.9895 val_accuracy:0.6440\n",
      "epoch:376 train_loss:0.9336 val_loss:0.9952 val_accuracy:0.6398\n",
      "epoch:377 train_loss:0.9256 val_loss:0.9853 val_accuracy:0.6449\n",
      "epoch:378 train_loss:0.9546 val_loss:0.9967 val_accuracy:0.6396\n",
      "epoch:379 train_loss:0.9441 val_loss:0.9943 val_accuracy:0.6405\n",
      "epoch:380 train_loss:0.9334 val_loss:0.9942 val_accuracy:0.6406\n",
      "epoch:381 train_loss:0.9394 val_loss:0.9836 val_accuracy:0.6486\n",
      "epoch:382 train_loss:0.9476 val_loss:0.9814 val_accuracy:0.6495\n",
      "epoch:383 train_loss:0.9407 val_loss:0.9855 val_accuracy:0.6468\n",
      "epoch:384 train_loss:0.9107 val_loss:1.0016 val_accuracy:0.6381\n",
      "epoch:385 train_loss:0.9602 val_loss:0.9814 val_accuracy:0.6495\n",
      "epoch:386 train_loss:0.9182 val_loss:0.9841 val_accuracy:0.6460\n",
      "epoch:387 train_loss:0.9417 val_loss:0.9845 val_accuracy:0.6448\n",
      "epoch:388 train_loss:0.9329 val_loss:0.9814 val_accuracy:0.6479\n",
      "epoch:389 train_loss:0.9454 val_loss:1.0005 val_accuracy:0.6384\n",
      "epoch:390 train_loss:0.9322 val_loss:0.9803 val_accuracy:0.6478\n",
      "epoch:391 train_loss:0.9448 val_loss:0.9980 val_accuracy:0.6395\n",
      "epoch:392 train_loss:0.9333 val_loss:0.9908 val_accuracy:0.6409\n",
      "epoch:393 train_loss:0.9308 val_loss:0.9820 val_accuracy:0.6480\n",
      "epoch:394 train_loss:0.9351 val_loss:0.9878 val_accuracy:0.6418\n",
      "epoch:395 train_loss:0.9124 val_loss:0.9837 val_accuracy:0.6444\n",
      "epoch:396 train_loss:0.9283 val_loss:0.9913 val_accuracy:0.6414\n",
      "epoch:397 train_loss:0.9445 val_loss:0.9794 val_accuracy:0.6519\n",
      "epoch:398 train_loss:0.9217 val_loss:0.9784 val_accuracy:0.6499\n",
      "epoch:399 train_loss:0.9358 val_loss:0.9966 val_accuracy:0.6397\n",
      "epoch:400 train_loss:0.9294 val_loss:0.9865 val_accuracy:0.6428\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer.backends import cuda\n",
    "from chainer.cuda import to_cpu\n",
    "\n",
    "gpu_id = 0 # CPUを用いる場合は、この値を-1にしてください\n",
    "\n",
    "batchsize = 100\n",
    "max_epoch = 400\n",
    "n_mid_units = 200\n",
    "\n",
    "xp = np\n",
    "if gpu_id >= 0:\n",
    "    xp = chainer.cuda.cupy\n",
    "\n",
    "reset_seed(0) # 乱数の種をセット\n",
    "\n",
    "train = np.load(\"MHT-train.npy\") # train: 450 x (framelen x 26 , framelen, maxphonenum)\n",
    "test = np.load(\"MHT-test.npy\")\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "\n",
    "net = MLP(n_mid_units=n_mid_units, n_out=nsymbol)\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.01).setup(net)\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    net.to_gpu(gpu_id)\n",
    "    \n",
    "while train_iter.epoch < max_epoch:\n",
    "\n",
    "    # ---------- 学習の1イテレーション ----------\n",
    "    train_batch = train_iter.next()\n",
    "\n",
    "    loss, _ = calculate_loss(net, train_batch, gpu_id, train_mode = True)\n",
    "\n",
    "    # 勾配の計算\n",
    "    net.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    # バッチ単位で古い記憶を削除し、計算コストを削減する。\n",
    "    loss.unchain_backward()\n",
    "\n",
    "    # パラメータの更新\n",
    "    optimizer.update()\n",
    "    \n",
    "    # --------------- ここまで ----------------\n",
    "\n",
    "    # 1エポック終了ごとにValidationデータに対する予測精度を測って、\n",
    "    # モデルの汎化性能が向上していることをチェックしよう\n",
    "    if train_iter.is_new_epoch:  # 1 epochが終わったら\n",
    "        # ロスの表示\n",
    "        print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "            train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "\n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "        \n",
    "        valid_loss, valid_accuracy = calculate_loss(net, test, gpu_id, train_mode=False)\n",
    "        valid_losses.append(to_cpu(valid_loss.array))\n",
    "        valid_accuracies.append(to_cpu(valid_accuracy.array))\n",
    "\n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            np.mean(valid_losses), np.mean(valid_accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pz_p01dG70Rn"
   },
   "source": [
    "学習が進むと、訓練データに対する損失が下がって行くのがわかります。テストデータに対する損失も同様に下がって行けば学習はうまく行っています。\n",
    "\n",
    "最終的には、フレームごとの音素認識率は65%程度になりました。\n",
    "\n",
    "## 課題\n",
    "1. 中間層ユニット数を変えて、認識精度がどのように変化するかを検討してください。\n",
    "1. 隠れ層の数を増やして、認識精度がどのように変化するかを検討してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRGac2m770Ro"
   },
   "source": [
    "## 音素認識結果の観察\n",
    "\n",
    "テストデータの0番目 (Jセット第1文) の各フレームに対して、学習したネットワークにより音素認識を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "96628xpN70Rq"
   },
   "outputs": [],
   "source": [
    "test_utterance_number = 0\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    net.to_gpu(gpu_id)\n",
    "\n",
    "Xs, ts, _ = converter(test, gpu_id)\n",
    "\n",
    "# テストデータを1つ取り出します\n",
    "X_test = Xs[test_utterance_number]\n",
    "t_test = ts[test_utterance_number]\n",
    "\n",
    "with chainer.using_config('train', False), \\\n",
    "     chainer.using_config('enable_backprop', False):\n",
    "    y_test = net([X_test])\n",
    "\n",
    "# Variable形式で出てくるので中身を取り出す\n",
    "y_test = y_test.array\n",
    "\n",
    "# 結果をCPUに送る\n",
    "y_test = to_cpu(y_test)\n",
    "t_test = to_cpu(t_test)\n",
    "\n",
    "# 予測確率の最大値のインデックスを見る\n",
    "pred_label = y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKoP_c5m70Rs"
   },
   "source": [
    "正解ラベルは `t_test` に、認識結果は `pred_label` に入っています。まず正解を確認しておきます。この発話の内容は、「小さな鰻屋に、熱気のようなものがみなぎる」です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1535004518493,
     "user": {
      "displayName": "Tomhiro Nagata",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116093945812520810263"
     },
     "user_tz": -540
    },
    "id": "yZRQVeMY70Rv",
    "outputId": "b0eb3875-f63f-4665-feff-768fb9f3efe9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch',\n",
       "       'ch', 'ch', 'ch', 'ch', 'ch', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 's', 's', 's', 's', 's', 's', 's', 's', 's',\n",
       "       's', 'a', 'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'a', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'u', 'u', 'u', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a',\n",
       "       'a', 'a', 'a', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'n',\n",
       "       'n', 'n', 'n', 'n', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e',\n",
       "       'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'k', 'k',\n",
       "       'k', 'k', 'k', 'i', 'i', 'i', 'i', 'i', 'n', 'n', 'n', 'n', 'n',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'y', 'y', 'y', 'y', 'y', 'y', 'y',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o',\n",
       "       'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'o', 'o', 'o', 'o', 'o', 'n', 'n', 'n',\n",
       "       'n', 'n', 'o', 'o', 'o', 'o', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
       "       'g', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'i', 'i', 'i', 'i', 'i', 'i', 'n', 'n',\n",
       "       'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a',\n",
       "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'r', 'r', 'r', 'r', 'r', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'u', 'u', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil'], dtype='<U3')"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EX0Imgy70Ry"
   },
   "source": [
    "以下は認識結果です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1009,
     "status": "ok",
     "timestamp": 1535004528119,
     "user": {
      "displayName": "Tomhiro Nagata",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116093945812520810263"
     },
     "user_tz": -540
    },
    "id": "RoioQ7yj70Rz",
    "outputId": "c427bcac-1def-458c-b25b-00bda1d6cba4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sil', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 't', 'u', 'I',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'ch', 'ch', 'ch', 'ch', 'ch',\n",
       "       'ch', 'ch', 'ch', 'sil', 'i', 'ky', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'j', 's', 's', 's', 's', 's', 's', 's', 's',\n",
       "       's', 'a', 'a', 'a', 'a', 'a', 'N', 'N', 'n', 'n', 'n', 'a', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'o', 'u', 'u', 'u', 'u', 'u', 'o',\n",
       "       'o', 'N', 'N', 'N', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a',\n",
       "       'a', 'e', 'e', 'e', 'N', 'g', 'n', 'g', 'g', 'g', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'e', 'e', 'e', 'e', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'r', 'n', 'n', 'n', 'n', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'sil', 'pau', 'pau', 'sil', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'sil', 'pau', 'sil', 'pau', 'pau', 'sil', 'sil', 'pau', 'pau',\n",
       "       'pau', 'pau', 'sil', 'pau', 'pau', 'pau', 'pau', 'pau', 'g', 'g',\n",
       "       'n', 'n', 'd', 'e', 'e', 'e', 'e', 'e', 'e', 'i', 'i', 'i', 'sil',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'sil', 'pau', 'k', 'ky', 'k', 'k', 'i', 'i', 'i', 'i', 'u', 'm',\n",
       "       'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'e', 'e',\n",
       "       'u', 'u', 'u', 'e', 'e', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o',\n",
       "       'o', 'o', 'o', 'o', 'N', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'o',\n",
       "       'o', 'o', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'o', 'o', 'o', 'o',\n",
       "       'o', 'u', 'N', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'N', 'N',\n",
       "       'N', 'N', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a',\n",
       "       'a', 'd', 'u', 'm', 'N', 'd', 'm', 'b', 'n', 'n', 'i', 'i', 'i',\n",
       "       'i', 'i', 'n', 'n', 'n', 'n', 'd', 'g', 'e', 'a', 'a', 'a', 'a',\n",
       "       'a', 'a', 'a', 'e', 'e', 'N', 'N', 'N', 'N', 'N', 'N', 'g', 'n',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'j', 'r', 'j', 'r',\n",
       "       'i', 'i', 'i', 'u', 'u', 'u', 'i', 'u', 'u', 'u', 'u', 'sil',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'sil', 'pau', 'pau', 'sil',\n",
       "       'sil', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau'], dtype='<U3')"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Lb66ZX970R5"
   },
   "source": [
    "まあまあ認識できているようです。\n",
    "\n",
    "しかし、少し変なところもあります。例えば sil という音素は必ず発話の先頭と末尾に現れ、そこ以外には来ないはずですが、途中に sil が現れたり、発話の先頭と末尾に pau (発話内ポーズ) が現れています。また、本来は1つの音素は数フレームにわたって連続して現れるのが普通ですが、この認識結果ではところどころ音素が激しく入れ替わっています。これは、ここで行っているフレーム単位の認識は、前後関係を一切考慮していないことが原因です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "khzA0PE170R5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FrameRecogFF.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
