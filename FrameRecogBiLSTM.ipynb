{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMを用いた連続音素認識\n",
    "\n",
    "RNN(再帰ニューラルネット)は、記憶を持つニューラルネットです。現時刻の隠れ層の出力を、次の時刻の入力と共に隠れ層に入力することにより、状態を保持できる仕組みです。\n",
    "\n",
    "LSTM (Long-Short Term Memory) はRNNの一種です。長期間の記憶を保持するために複雑な機構を有しています。今回は、時間的に順方向の記憶と逆方向の記憶を別個のユニットで保持する双方向LSTM (Bi-directional LSTM) を使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワーク構造の記述\n",
    "\n",
    "今回のネットワークは、入力層から1番目の隠れ層が線形変換+ReLU, 次が双方向LSTM, 最後に出力層まで線形変換、という構造を持っています。`n_lstm_layers`は、双方向LSTMを何層重ねるかのパラメータです。\n",
    "\n",
    "双方向LSTMの隠れユニットは順方向と逆方向の2組あるので、LSTMから出力層への結合を表す`l3`は $2H\\times D$ 行列になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/hiroki/conda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "class RNN(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_lstm_layers=1, n_mid_units=100, n_out=41, dropout=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # パラメータを持つ層の登録\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_mid_units)\n",
    "            self.l2 = L.NStepBiLSTM(n_lstm_layers, n_mid_units, n_mid_units, dropout)\n",
    "            self.l3 = L.Linear(n_mid_units * 2, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # データを受け取った際のforward計算を書く\n",
    "        numframes = [X.shape[0] for X in x]\n",
    "        split_point = np.cumsum(numframes)[:-1]\n",
    "        x1 = F.concat(x, axis=0)\n",
    "        h1 = F.relu(self.l1(x1))\n",
    "        h1 = F.split_axis(h1, split_point, axis=0)\n",
    "        hy, cy, ys = self.l2(None, None, h1)\n",
    "        h2 = F.concat(ys, axis=0)\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下はfeed-forwardと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.dataset import to_device\n",
    "from chainer.dataset import concat_examples\n",
    "import numpy as np\n",
    "\n",
    "def converter(batch, device):\n",
    "    # alternative to chainer.dataset.concat_examples\n",
    "    \n",
    "    Xs = [to_device(device, X) for X, _, __ in batch]\n",
    "    ts = [to_device(device, t) for _, t, __ in batch]\n",
    "\n",
    "    lab_batch = [lab.astype(np.int32) for _, __, lab in batch]\n",
    "    labs = concat_examples(lab_batch, device, padding=0)\n",
    "    \n",
    "    return Xs, ts, labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer.backends import cuda\n",
    "\n",
    "def calculate_loss(net, batch, gpu_id=0, train_mode=True):\n",
    "    numframes = [X.shape[0] for (X, t, lab) in batch]\n",
    "    \n",
    "    Xs, ts, _ = converter(batch, gpu_id) # 生ラベルは使わない\n",
    "\n",
    "    with chainer.using_config('train', train_mode), \\\n",
    "         chainer.using_config('enable_backprop', train_mode):\n",
    "        ys = net(Xs)\n",
    "    \n",
    "    ts = F.concat(ts, axis=0)\n",
    "    ts = ts.reshape(ts.shape[0],)\n",
    "    \n",
    "    # ロスを計算\n",
    "    loss = F.softmax_cross_entropy(ys, ts)\n",
    "\n",
    "    # 精度を計算\n",
    "    accuracy = F.accuracy(ys, ts)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import chainer\n",
    "\n",
    "def reset_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if gpu_id >= 0:\n",
    "        chainer.cuda.cupy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "with open('phones') as f:\n",
    "    phones = f.read().splitlines()\n",
    "le = LabelEncoder()\n",
    "le.fit(phones)\n",
    "nsymbol = len(phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの学習\n",
    "`MLP` のかわりに `RNN` でネットワークを作っているところだけが違います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:01 train_loss:2.4433 val_loss:2.3563 val_accuracy:0.4315\n",
      "epoch:02 train_loss:2.1308 val_loss:2.0579 val_accuracy:0.4634\n",
      "epoch:03 train_loss:1.8794 val_loss:1.8578 val_accuracy:0.4962\n",
      "epoch:04 train_loss:1.7482 val_loss:1.7381 val_accuracy:0.5402\n",
      "epoch:05 train_loss:1.6392 val_loss:1.6325 val_accuracy:0.5781\n",
      "epoch:06 train_loss:1.5588 val_loss:1.5595 val_accuracy:0.6007\n",
      "epoch:07 train_loss:1.5019 val_loss:1.4961 val_accuracy:0.5743\n",
      "epoch:08 train_loss:1.4267 val_loss:1.4424 val_accuracy:0.5850\n",
      "epoch:09 train_loss:1.3819 val_loss:1.3746 val_accuracy:0.6503\n",
      "epoch:10 train_loss:1.3176 val_loss:1.3318 val_accuracy:0.6330\n",
      "epoch:11 train_loss:1.3260 val_loss:1.3045 val_accuracy:0.6312\n",
      "epoch:12 train_loss:1.2641 val_loss:1.2576 val_accuracy:0.6661\n",
      "epoch:13 train_loss:1.2111 val_loss:1.2156 val_accuracy:0.6648\n",
      "epoch:14 train_loss:1.1585 val_loss:1.2158 val_accuracy:0.6189\n",
      "epoch:15 train_loss:1.1544 val_loss:1.1507 val_accuracy:0.6960\n",
      "epoch:16 train_loss:1.1165 val_loss:1.1216 val_accuracy:0.7065\n",
      "epoch:17 train_loss:1.1169 val_loss:1.1060 val_accuracy:0.6956\n",
      "epoch:18 train_loss:1.0665 val_loss:1.0843 val_accuracy:0.6970\n",
      "epoch:19 train_loss:1.0674 val_loss:1.0658 val_accuracy:0.6984\n",
      "epoch:20 train_loss:1.0317 val_loss:1.0319 val_accuracy:0.7248\n",
      "epoch:21 train_loss:1.0031 val_loss:1.0156 val_accuracy:0.7181\n",
      "epoch:22 train_loss:1.0027 val_loss:1.0075 val_accuracy:0.7130\n",
      "epoch:23 train_loss:0.9712 val_loss:0.9796 val_accuracy:0.7304\n",
      "epoch:24 train_loss:0.9499 val_loss:0.9666 val_accuracy:0.7340\n",
      "epoch:25 train_loss:0.9365 val_loss:0.9868 val_accuracy:0.6987\n",
      "epoch:26 train_loss:0.9325 val_loss:0.9433 val_accuracy:0.7394\n",
      "epoch:27 train_loss:0.9022 val_loss:0.9092 val_accuracy:0.7538\n",
      "epoch:28 train_loss:0.8877 val_loss:0.8972 val_accuracy:0.7561\n",
      "epoch:29 train_loss:0.8885 val_loss:0.8949 val_accuracy:0.7467\n",
      "epoch:30 train_loss:0.8537 val_loss:0.8798 val_accuracy:0.7531\n",
      "epoch:31 train_loss:0.8417 val_loss:0.8627 val_accuracy:0.7649\n",
      "epoch:32 train_loss:0.8184 val_loss:0.8597 val_accuracy:0.7650\n",
      "epoch:33 train_loss:0.8233 val_loss:0.8499 val_accuracy:0.7614\n",
      "epoch:34 train_loss:0.8064 val_loss:0.8411 val_accuracy:0.7639\n",
      "epoch:35 train_loss:0.8030 val_loss:0.8190 val_accuracy:0.7740\n",
      "epoch:36 train_loss:0.7857 val_loss:0.8184 val_accuracy:0.7751\n",
      "epoch:37 train_loss:0.7931 val_loss:0.8053 val_accuracy:0.7750\n",
      "epoch:38 train_loss:0.7476 val_loss:0.7961 val_accuracy:0.7770\n",
      "epoch:39 train_loss:0.7682 val_loss:0.7898 val_accuracy:0.7822\n",
      "epoch:40 train_loss:0.7587 val_loss:0.7774 val_accuracy:0.7869\n",
      "epoch:41 train_loss:0.7481 val_loss:0.7748 val_accuracy:0.7795\n",
      "epoch:42 train_loss:0.7290 val_loss:0.7617 val_accuracy:0.7885\n",
      "epoch:43 train_loss:0.7071 val_loss:0.7604 val_accuracy:0.7902\n",
      "epoch:44 train_loss:0.7208 val_loss:0.7482 val_accuracy:0.7930\n",
      "epoch:45 train_loss:0.7031 val_loss:0.7458 val_accuracy:0.7914\n",
      "epoch:46 train_loss:0.6937 val_loss:0.7417 val_accuracy:0.7930\n",
      "epoch:47 train_loss:0.6992 val_loss:0.7320 val_accuracy:0.7925\n",
      "epoch:48 train_loss:0.6873 val_loss:0.7239 val_accuracy:0.7966\n",
      "epoch:49 train_loss:0.6959 val_loss:0.7187 val_accuracy:0.7953\n",
      "epoch:50 train_loss:0.6779 val_loss:0.7117 val_accuracy:0.7974\n",
      "epoch:51 train_loss:0.6555 val_loss:0.6992 val_accuracy:0.8059\n",
      "epoch:52 train_loss:0.6726 val_loss:0.6963 val_accuracy:0.8037\n",
      "epoch:53 train_loss:0.6634 val_loss:0.6915 val_accuracy:0.8019\n",
      "epoch:54 train_loss:0.6504 val_loss:0.6876 val_accuracy:0.8032\n",
      "epoch:55 train_loss:0.6431 val_loss:0.6915 val_accuracy:0.8068\n",
      "epoch:56 train_loss:0.6445 val_loss:0.6796 val_accuracy:0.8094\n",
      "epoch:57 train_loss:0.6268 val_loss:0.6687 val_accuracy:0.8103\n",
      "epoch:58 train_loss:0.6376 val_loss:0.6724 val_accuracy:0.8065\n",
      "epoch:59 train_loss:0.6248 val_loss:0.6629 val_accuracy:0.8119\n",
      "epoch:60 train_loss:0.6274 val_loss:0.6567 val_accuracy:0.8148\n",
      "epoch:61 train_loss:0.6300 val_loss:0.6592 val_accuracy:0.8083\n",
      "epoch:62 train_loss:0.6135 val_loss:0.6530 val_accuracy:0.8138\n",
      "epoch:63 train_loss:0.5871 val_loss:0.6493 val_accuracy:0.8139\n",
      "epoch:64 train_loss:0.6037 val_loss:0.6399 val_accuracy:0.8174\n",
      "epoch:65 train_loss:0.5961 val_loss:0.6317 val_accuracy:0.8201\n",
      "epoch:66 train_loss:0.5776 val_loss:0.6311 val_accuracy:0.8194\n",
      "epoch:67 train_loss:0.5876 val_loss:0.6270 val_accuracy:0.8209\n",
      "epoch:68 train_loss:0.5821 val_loss:0.6288 val_accuracy:0.8217\n",
      "epoch:69 train_loss:0.5634 val_loss:0.6162 val_accuracy:0.8233\n",
      "epoch:70 train_loss:0.5788 val_loss:0.6222 val_accuracy:0.8208\n",
      "epoch:71 train_loss:0.5615 val_loss:0.6202 val_accuracy:0.8206\n",
      "epoch:72 train_loss:0.5660 val_loss:0.6146 val_accuracy:0.8244\n",
      "epoch:73 train_loss:0.5602 val_loss:0.6041 val_accuracy:0.8277\n",
      "epoch:74 train_loss:0.5703 val_loss:0.5997 val_accuracy:0.8288\n",
      "epoch:75 train_loss:0.5531 val_loss:0.6036 val_accuracy:0.8236\n",
      "epoch:76 train_loss:0.5621 val_loss:0.6173 val_accuracy:0.8182\n",
      "epoch:77 train_loss:0.5566 val_loss:0.5937 val_accuracy:0.8300\n",
      "epoch:78 train_loss:0.5349 val_loss:0.5915 val_accuracy:0.8305\n",
      "epoch:79 train_loss:0.5436 val_loss:0.5958 val_accuracy:0.8234\n",
      "epoch:80 train_loss:0.5480 val_loss:0.5817 val_accuracy:0.8290\n",
      "epoch:81 train_loss:0.5432 val_loss:0.5809 val_accuracy:0.8319\n",
      "epoch:82 train_loss:0.5230 val_loss:0.5817 val_accuracy:0.8320\n",
      "epoch:83 train_loss:0.5261 val_loss:0.5803 val_accuracy:0.8279\n",
      "epoch:84 train_loss:0.5263 val_loss:0.5697 val_accuracy:0.8332\n",
      "epoch:85 train_loss:0.5077 val_loss:0.5708 val_accuracy:0.8316\n",
      "epoch:86 train_loss:0.5191 val_loss:0.5680 val_accuracy:0.8330\n",
      "epoch:87 train_loss:0.5066 val_loss:0.5589 val_accuracy:0.8363\n",
      "epoch:88 train_loss:0.5001 val_loss:0.5601 val_accuracy:0.8351\n",
      "epoch:89 train_loss:0.5177 val_loss:0.5610 val_accuracy:0.8362\n",
      "epoch:90 train_loss:0.4958 val_loss:0.5480 val_accuracy:0.8412\n",
      "epoch:91 train_loss:0.5028 val_loss:0.5485 val_accuracy:0.8393\n",
      "epoch:92 train_loss:0.4972 val_loss:0.5569 val_accuracy:0.8370\n",
      "epoch:93 train_loss:0.4973 val_loss:0.5548 val_accuracy:0.8367\n",
      "epoch:94 train_loss:0.4920 val_loss:0.5526 val_accuracy:0.8363\n",
      "epoch:95 train_loss:0.5045 val_loss:0.5456 val_accuracy:0.8404\n",
      "epoch:96 train_loss:0.4860 val_loss:0.5392 val_accuracy:0.8417\n",
      "epoch:97 train_loss:0.5017 val_loss:0.5421 val_accuracy:0.8384\n",
      "epoch:98 train_loss:0.4888 val_loss:0.5420 val_accuracy:0.8397\n",
      "epoch:99 train_loss:0.4875 val_loss:0.5286 val_accuracy:0.8446\n",
      "epoch:100 train_loss:0.4817 val_loss:0.5309 val_accuracy:0.8449\n",
      "epoch:101 train_loss:0.4721 val_loss:0.5316 val_accuracy:0.8445\n",
      "epoch:102 train_loss:0.4699 val_loss:0.5243 val_accuracy:0.8461\n",
      "epoch:103 train_loss:0.4586 val_loss:0.5238 val_accuracy:0.8457\n",
      "epoch:104 train_loss:0.4581 val_loss:0.5240 val_accuracy:0.8461\n",
      "epoch:105 train_loss:0.4890 val_loss:0.5347 val_accuracy:0.8394\n",
      "epoch:106 train_loss:0.4526 val_loss:0.5289 val_accuracy:0.8431\n",
      "epoch:107 train_loss:0.4786 val_loss:0.5174 val_accuracy:0.8492\n",
      "epoch:108 train_loss:0.4642 val_loss:0.5223 val_accuracy:0.8456\n",
      "epoch:109 train_loss:0.4668 val_loss:0.5188 val_accuracy:0.8468\n",
      "epoch:110 train_loss:0.4662 val_loss:0.5097 val_accuracy:0.8498\n",
      "epoch:111 train_loss:0.4625 val_loss:0.5112 val_accuracy:0.8510\n",
      "epoch:112 train_loss:0.4700 val_loss:0.5189 val_accuracy:0.8451\n",
      "epoch:113 train_loss:0.4415 val_loss:0.5077 val_accuracy:0.8497\n",
      "epoch:114 train_loss:0.4487 val_loss:0.5009 val_accuracy:0.8523\n",
      "epoch:115 train_loss:0.4465 val_loss:0.5024 val_accuracy:0.8503\n",
      "epoch:116 train_loss:0.4377 val_loss:0.5006 val_accuracy:0.8519\n",
      "epoch:117 train_loss:0.4516 val_loss:0.5107 val_accuracy:0.8460\n",
      "epoch:118 train_loss:0.4578 val_loss:0.5085 val_accuracy:0.8495\n",
      "epoch:119 train_loss:0.4328 val_loss:0.4927 val_accuracy:0.8539\n",
      "epoch:120 train_loss:0.4384 val_loss:0.4927 val_accuracy:0.8535\n",
      "epoch:121 train_loss:0.4200 val_loss:0.4955 val_accuracy:0.8530\n",
      "epoch:122 train_loss:0.4437 val_loss:0.4918 val_accuracy:0.8533\n",
      "epoch:123 train_loss:0.4348 val_loss:0.4935 val_accuracy:0.8527\n",
      "epoch:124 train_loss:0.4398 val_loss:0.4885 val_accuracy:0.8550\n",
      "epoch:125 train_loss:0.4417 val_loss:0.4887 val_accuracy:0.8519\n",
      "epoch:126 train_loss:0.4375 val_loss:0.4837 val_accuracy:0.8556\n",
      "epoch:127 train_loss:0.4231 val_loss:0.4874 val_accuracy:0.8551\n",
      "epoch:128 train_loss:0.4239 val_loss:0.4844 val_accuracy:0.8553\n",
      "epoch:129 train_loss:0.4345 val_loss:0.4827 val_accuracy:0.8569\n",
      "epoch:130 train_loss:0.4386 val_loss:0.4910 val_accuracy:0.8534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:131 train_loss:0.4174 val_loss:0.4775 val_accuracy:0.8557\n",
      "epoch:132 train_loss:0.4162 val_loss:0.4762 val_accuracy:0.8565\n",
      "epoch:133 train_loss:0.4179 val_loss:0.4739 val_accuracy:0.8586\n",
      "epoch:134 train_loss:0.4291 val_loss:0.4751 val_accuracy:0.8584\n",
      "epoch:135 train_loss:0.4213 val_loss:0.4706 val_accuracy:0.8588\n",
      "epoch:136 train_loss:0.4216 val_loss:0.4668 val_accuracy:0.8600\n",
      "epoch:137 train_loss:0.4244 val_loss:0.4730 val_accuracy:0.8580\n",
      "epoch:138 train_loss:0.4167 val_loss:0.4711 val_accuracy:0.8595\n",
      "epoch:139 train_loss:0.4427 val_loss:0.4865 val_accuracy:0.8537\n",
      "epoch:140 train_loss:0.4126 val_loss:0.4745 val_accuracy:0.8574\n",
      "epoch:141 train_loss:0.4192 val_loss:0.4681 val_accuracy:0.8603\n",
      "epoch:142 train_loss:0.4056 val_loss:0.4605 val_accuracy:0.8642\n",
      "epoch:143 train_loss:0.3885 val_loss:0.4661 val_accuracy:0.8597\n",
      "epoch:144 train_loss:0.4118 val_loss:0.4673 val_accuracy:0.8602\n",
      "epoch:145 train_loss:0.3861 val_loss:0.4598 val_accuracy:0.8630\n",
      "epoch:146 train_loss:0.4057 val_loss:0.4587 val_accuracy:0.8613\n",
      "epoch:147 train_loss:0.4001 val_loss:0.4588 val_accuracy:0.8613\n",
      "epoch:148 train_loss:0.3923 val_loss:0.4592 val_accuracy:0.8617\n",
      "epoch:149 train_loss:0.3886 val_loss:0.4587 val_accuracy:0.8597\n",
      "epoch:150 train_loss:0.3948 val_loss:0.4562 val_accuracy:0.8611\n",
      "epoch:151 train_loss:0.3906 val_loss:0.4559 val_accuracy:0.8639\n",
      "epoch:152 train_loss:0.3695 val_loss:0.4497 val_accuracy:0.8652\n",
      "epoch:153 train_loss:0.3855 val_loss:0.4460 val_accuracy:0.8665\n",
      "epoch:154 train_loss:0.3861 val_loss:0.4492 val_accuracy:0.8652\n",
      "epoch:155 train_loss:0.3882 val_loss:0.4500 val_accuracy:0.8652\n",
      "epoch:156 train_loss:0.3794 val_loss:0.4518 val_accuracy:0.8648\n",
      "epoch:157 train_loss:0.3820 val_loss:0.4421 val_accuracy:0.8656\n",
      "epoch:158 train_loss:0.3903 val_loss:0.4457 val_accuracy:0.8651\n",
      "epoch:159 train_loss:0.3768 val_loss:0.4542 val_accuracy:0.8623\n",
      "epoch:160 train_loss:0.3824 val_loss:0.4536 val_accuracy:0.8613\n",
      "epoch:161 train_loss:0.3851 val_loss:0.4474 val_accuracy:0.8644\n",
      "epoch:162 train_loss:0.3875 val_loss:0.4369 val_accuracy:0.8677\n",
      "epoch:163 train_loss:0.3811 val_loss:0.4434 val_accuracy:0.8662\n",
      "epoch:164 train_loss:0.3735 val_loss:0.4488 val_accuracy:0.8653\n",
      "epoch:165 train_loss:0.3637 val_loss:0.4397 val_accuracy:0.8656\n",
      "epoch:166 train_loss:0.3697 val_loss:0.4357 val_accuracy:0.8683\n",
      "epoch:167 train_loss:0.3617 val_loss:0.4413 val_accuracy:0.8644\n",
      "epoch:168 train_loss:0.3730 val_loss:0.4341 val_accuracy:0.8684\n",
      "epoch:169 train_loss:0.3737 val_loss:0.4388 val_accuracy:0.8661\n",
      "epoch:170 train_loss:0.3628 val_loss:0.4336 val_accuracy:0.8667\n",
      "epoch:171 train_loss:0.3731 val_loss:0.4420 val_accuracy:0.8651\n",
      "epoch:172 train_loss:0.3744 val_loss:0.4414 val_accuracy:0.8659\n",
      "epoch:173 train_loss:0.3538 val_loss:0.4264 val_accuracy:0.8718\n",
      "epoch:174 train_loss:0.3720 val_loss:0.4298 val_accuracy:0.8710\n",
      "epoch:175 train_loss:0.3751 val_loss:0.4304 val_accuracy:0.8690\n",
      "epoch:176 train_loss:0.3720 val_loss:0.4305 val_accuracy:0.8698\n",
      "epoch:177 train_loss:0.3548 val_loss:0.4229 val_accuracy:0.8703\n",
      "epoch:178 train_loss:0.3668 val_loss:0.4227 val_accuracy:0.8720\n",
      "epoch:179 train_loss:0.3616 val_loss:0.4348 val_accuracy:0.8671\n",
      "epoch:180 train_loss:0.3554 val_loss:0.4318 val_accuracy:0.8688\n",
      "epoch:181 train_loss:0.3616 val_loss:0.4318 val_accuracy:0.8678\n",
      "epoch:182 train_loss:0.3739 val_loss:0.4317 val_accuracy:0.8686\n",
      "epoch:183 train_loss:0.3620 val_loss:0.4187 val_accuracy:0.8720\n",
      "epoch:184 train_loss:0.3524 val_loss:0.4180 val_accuracy:0.8727\n",
      "epoch:185 train_loss:0.3667 val_loss:0.4231 val_accuracy:0.8703\n",
      "epoch:186 train_loss:0.3409 val_loss:0.4214 val_accuracy:0.8701\n",
      "epoch:187 train_loss:0.3495 val_loss:0.4177 val_accuracy:0.8716\n",
      "epoch:188 train_loss:0.3609 val_loss:0.4205 val_accuracy:0.8702\n",
      "epoch:189 train_loss:0.3497 val_loss:0.4203 val_accuracy:0.8713\n",
      "epoch:190 train_loss:0.3474 val_loss:0.4208 val_accuracy:0.8698\n",
      "epoch:191 train_loss:0.3433 val_loss:0.4278 val_accuracy:0.8674\n",
      "epoch:192 train_loss:0.3496 val_loss:0.4213 val_accuracy:0.8705\n",
      "epoch:193 train_loss:0.3548 val_loss:0.4118 val_accuracy:0.8741\n",
      "epoch:194 train_loss:0.3396 val_loss:0.4112 val_accuracy:0.8739\n",
      "epoch:195 train_loss:0.3370 val_loss:0.4160 val_accuracy:0.8717\n",
      "epoch:196 train_loss:0.3471 val_loss:0.4181 val_accuracy:0.8707\n",
      "epoch:197 train_loss:0.3575 val_loss:0.4133 val_accuracy:0.8734\n",
      "epoch:198 train_loss:0.3308 val_loss:0.4077 val_accuracy:0.8745\n",
      "epoch:199 train_loss:0.3328 val_loss:0.4117 val_accuracy:0.8742\n",
      "epoch:200 train_loss:0.3327 val_loss:0.4084 val_accuracy:0.8737\n",
      "epoch:201 train_loss:0.3346 val_loss:0.4192 val_accuracy:0.8708\n",
      "epoch:202 train_loss:0.3408 val_loss:0.4189 val_accuracy:0.8707\n",
      "epoch:203 train_loss:0.3430 val_loss:0.4152 val_accuracy:0.8710\n",
      "epoch:204 train_loss:0.3330 val_loss:0.4069 val_accuracy:0.8752\n",
      "epoch:205 train_loss:0.3499 val_loss:0.4176 val_accuracy:0.8701\n",
      "epoch:206 train_loss:0.3412 val_loss:0.4148 val_accuracy:0.8719\n",
      "epoch:207 train_loss:0.3453 val_loss:0.4094 val_accuracy:0.8729\n",
      "epoch:208 train_loss:0.3490 val_loss:0.4036 val_accuracy:0.8750\n",
      "epoch:209 train_loss:0.3221 val_loss:0.3982 val_accuracy:0.8781\n",
      "epoch:210 train_loss:0.3252 val_loss:0.3947 val_accuracy:0.8789\n",
      "epoch:211 train_loss:0.3290 val_loss:0.4048 val_accuracy:0.8747\n",
      "epoch:212 train_loss:0.3397 val_loss:0.4180 val_accuracy:0.8709\n",
      "epoch:213 train_loss:0.3457 val_loss:0.3947 val_accuracy:0.8778\n",
      "epoch:214 train_loss:0.3385 val_loss:0.4027 val_accuracy:0.8731\n",
      "epoch:215 train_loss:0.3351 val_loss:0.4104 val_accuracy:0.8727\n",
      "epoch:216 train_loss:0.3513 val_loss:0.4082 val_accuracy:0.8713\n",
      "epoch:217 train_loss:0.3324 val_loss:0.3969 val_accuracy:0.8784\n",
      "epoch:218 train_loss:0.3171 val_loss:0.3957 val_accuracy:0.8767\n",
      "epoch:219 train_loss:0.3283 val_loss:0.3959 val_accuracy:0.8758\n",
      "epoch:220 train_loss:0.3312 val_loss:0.3924 val_accuracy:0.8778\n",
      "epoch:221 train_loss:0.3278 val_loss:0.4145 val_accuracy:0.8712\n",
      "epoch:222 train_loss:0.3195 val_loss:0.4030 val_accuracy:0.8740\n",
      "epoch:223 train_loss:0.3138 val_loss:0.3890 val_accuracy:0.8791\n",
      "epoch:224 train_loss:0.3164 val_loss:0.3912 val_accuracy:0.8771\n",
      "epoch:225 train_loss:0.3123 val_loss:0.3947 val_accuracy:0.8794\n",
      "epoch:226 train_loss:0.3167 val_loss:0.3940 val_accuracy:0.8778\n",
      "epoch:227 train_loss:0.3226 val_loss:0.3931 val_accuracy:0.8760\n",
      "epoch:228 train_loss:0.3175 val_loss:0.3972 val_accuracy:0.8768\n",
      "epoch:229 train_loss:0.3290 val_loss:0.3932 val_accuracy:0.8772\n",
      "epoch:230 train_loss:0.3059 val_loss:0.3900 val_accuracy:0.8784\n",
      "epoch:231 train_loss:0.3131 val_loss:0.3904 val_accuracy:0.8789\n",
      "epoch:232 train_loss:0.3201 val_loss:0.3928 val_accuracy:0.8778\n",
      "epoch:233 train_loss:0.3036 val_loss:0.3862 val_accuracy:0.8790\n",
      "epoch:234 train_loss:0.3166 val_loss:0.3890 val_accuracy:0.8774\n",
      "epoch:235 train_loss:0.3136 val_loss:0.3939 val_accuracy:0.8773\n",
      "epoch:236 train_loss:0.3179 val_loss:0.3941 val_accuracy:0.8783\n",
      "epoch:237 train_loss:0.3229 val_loss:0.3886 val_accuracy:0.8788\n",
      "epoch:238 train_loss:0.3095 val_loss:0.3872 val_accuracy:0.8792\n",
      "epoch:239 train_loss:0.3076 val_loss:0.3905 val_accuracy:0.8775\n",
      "epoch:240 train_loss:0.3225 val_loss:0.3894 val_accuracy:0.8775\n",
      "epoch:241 train_loss:0.3114 val_loss:0.3800 val_accuracy:0.8815\n",
      "epoch:242 train_loss:0.3097 val_loss:0.3742 val_accuracy:0.8828\n",
      "epoch:243 train_loss:0.2969 val_loss:0.3803 val_accuracy:0.8815\n",
      "epoch:244 train_loss:0.3187 val_loss:0.3913 val_accuracy:0.8770\n",
      "epoch:245 train_loss:0.2872 val_loss:0.3779 val_accuracy:0.8838\n",
      "epoch:246 train_loss:0.3087 val_loss:0.3880 val_accuracy:0.8776\n",
      "epoch:247 train_loss:0.3023 val_loss:0.3761 val_accuracy:0.8836\n",
      "epoch:248 train_loss:0.3161 val_loss:0.3894 val_accuracy:0.8776\n",
      "epoch:249 train_loss:0.2966 val_loss:0.3766 val_accuracy:0.8829\n",
      "epoch:250 train_loss:0.3021 val_loss:0.3737 val_accuracy:0.8855\n",
      "epoch:251 train_loss:0.3015 val_loss:0.3806 val_accuracy:0.8807\n",
      "epoch:252 train_loss:0.3082 val_loss:0.3826 val_accuracy:0.8791\n",
      "epoch:253 train_loss:0.3016 val_loss:0.3773 val_accuracy:0.8811\n",
      "epoch:254 train_loss:0.2962 val_loss:0.3773 val_accuracy:0.8838\n",
      "epoch:255 train_loss:0.3103 val_loss:0.3772 val_accuracy:0.8812\n",
      "epoch:256 train_loss:0.2938 val_loss:0.3804 val_accuracy:0.8801\n",
      "epoch:257 train_loss:0.2918 val_loss:0.3759 val_accuracy:0.8837\n",
      "epoch:258 train_loss:0.2973 val_loss:0.3830 val_accuracy:0.8813\n",
      "epoch:259 train_loss:0.3007 val_loss:0.3701 val_accuracy:0.8823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:260 train_loss:0.3044 val_loss:0.3695 val_accuracy:0.8846\n",
      "epoch:261 train_loss:0.3013 val_loss:0.3795 val_accuracy:0.8817\n",
      "epoch:262 train_loss:0.3041 val_loss:0.3783 val_accuracy:0.8809\n",
      "epoch:263 train_loss:0.2980 val_loss:0.3717 val_accuracy:0.8835\n",
      "epoch:264 train_loss:0.2978 val_loss:0.3720 val_accuracy:0.8845\n",
      "epoch:265 train_loss:0.2873 val_loss:0.3822 val_accuracy:0.8799\n",
      "epoch:266 train_loss:0.2973 val_loss:0.3790 val_accuracy:0.8803\n",
      "epoch:267 train_loss:0.2916 val_loss:0.3774 val_accuracy:0.8810\n",
      "epoch:268 train_loss:0.2928 val_loss:0.3779 val_accuracy:0.8811\n",
      "epoch:269 train_loss:0.3018 val_loss:0.3655 val_accuracy:0.8867\n",
      "epoch:270 train_loss:0.3033 val_loss:0.3684 val_accuracy:0.8859\n",
      "epoch:271 train_loss:0.2749 val_loss:0.3702 val_accuracy:0.8829\n",
      "epoch:272 train_loss:0.2928 val_loss:0.3720 val_accuracy:0.8815\n",
      "epoch:273 train_loss:0.2885 val_loss:0.3735 val_accuracy:0.8846\n",
      "epoch:274 train_loss:0.2852 val_loss:0.3714 val_accuracy:0.8863\n",
      "epoch:275 train_loss:0.2829 val_loss:0.3650 val_accuracy:0.8867\n",
      "epoch:276 train_loss:0.3014 val_loss:0.3632 val_accuracy:0.8861\n",
      "epoch:277 train_loss:0.2861 val_loss:0.3722 val_accuracy:0.8831\n",
      "epoch:278 train_loss:0.2766 val_loss:0.3694 val_accuracy:0.8826\n",
      "epoch:279 train_loss:0.2861 val_loss:0.3658 val_accuracy:0.8847\n",
      "epoch:280 train_loss:0.2922 val_loss:0.3681 val_accuracy:0.8841\n",
      "epoch:281 train_loss:0.2775 val_loss:0.3621 val_accuracy:0.8882\n",
      "epoch:282 train_loss:0.2783 val_loss:0.3707 val_accuracy:0.8845\n",
      "epoch:283 train_loss:0.2756 val_loss:0.3578 val_accuracy:0.8893\n",
      "epoch:284 train_loss:0.2831 val_loss:0.3601 val_accuracy:0.8858\n",
      "epoch:285 train_loss:0.2713 val_loss:0.3622 val_accuracy:0.8882\n",
      "epoch:286 train_loss:0.2887 val_loss:0.3615 val_accuracy:0.8878\n",
      "epoch:287 train_loss:0.2691 val_loss:0.3632 val_accuracy:0.8849\n",
      "epoch:288 train_loss:0.2838 val_loss:0.3608 val_accuracy:0.8863\n",
      "epoch:289 train_loss:0.2848 val_loss:0.3591 val_accuracy:0.8895\n",
      "epoch:290 train_loss:0.2841 val_loss:0.3597 val_accuracy:0.8901\n",
      "epoch:291 train_loss:0.2781 val_loss:0.3607 val_accuracy:0.8851\n",
      "epoch:292 train_loss:0.2795 val_loss:0.3634 val_accuracy:0.8852\n",
      "epoch:293 train_loss:0.2840 val_loss:0.3498 val_accuracy:0.8910\n",
      "epoch:294 train_loss:0.2781 val_loss:0.3542 val_accuracy:0.8894\n",
      "epoch:295 train_loss:0.2754 val_loss:0.3838 val_accuracy:0.8781\n",
      "epoch:296 train_loss:0.2858 val_loss:0.3677 val_accuracy:0.8855\n",
      "epoch:297 train_loss:0.2709 val_loss:0.3539 val_accuracy:0.8892\n",
      "epoch:298 train_loss:0.2809 val_loss:0.3539 val_accuracy:0.8885\n",
      "epoch:299 train_loss:0.2748 val_loss:0.3562 val_accuracy:0.8880\n",
      "epoch:300 train_loss:0.2777 val_loss:0.3558 val_accuracy:0.8880\n",
      "epoch:301 train_loss:0.2626 val_loss:0.3535 val_accuracy:0.8918\n",
      "epoch:302 train_loss:0.2826 val_loss:0.3598 val_accuracy:0.8904\n",
      "epoch:303 train_loss:0.2777 val_loss:0.3609 val_accuracy:0.8858\n",
      "epoch:304 train_loss:0.2661 val_loss:0.3529 val_accuracy:0.8882\n",
      "epoch:305 train_loss:0.2715 val_loss:0.3551 val_accuracy:0.8874\n",
      "epoch:306 train_loss:0.2759 val_loss:0.3528 val_accuracy:0.8892\n",
      "epoch:307 train_loss:0.2665 val_loss:0.3611 val_accuracy:0.8851\n",
      "epoch:308 train_loss:0.2777 val_loss:0.3603 val_accuracy:0.8855\n",
      "epoch:309 train_loss:0.2719 val_loss:0.3600 val_accuracy:0.8851\n",
      "epoch:310 train_loss:0.2725 val_loss:0.3562 val_accuracy:0.8884\n",
      "epoch:311 train_loss:0.2771 val_loss:0.3488 val_accuracy:0.8900\n",
      "epoch:312 train_loss:0.2582 val_loss:0.3449 val_accuracy:0.8929\n",
      "epoch:313 train_loss:0.2583 val_loss:0.3508 val_accuracy:0.8924\n",
      "epoch:314 train_loss:0.2680 val_loss:0.3480 val_accuracy:0.8926\n",
      "epoch:315 train_loss:0.2745 val_loss:0.3561 val_accuracy:0.8874\n",
      "epoch:316 train_loss:0.2684 val_loss:0.3555 val_accuracy:0.8871\n",
      "epoch:317 train_loss:0.2632 val_loss:0.3430 val_accuracy:0.8937\n",
      "epoch:318 train_loss:0.2571 val_loss:0.3410 val_accuracy:0.8950\n",
      "epoch:319 train_loss:0.2704 val_loss:0.3643 val_accuracy:0.8817\n",
      "epoch:320 train_loss:0.2783 val_loss:0.3662 val_accuracy:0.8812\n",
      "epoch:321 train_loss:0.2654 val_loss:0.3502 val_accuracy:0.8916\n",
      "epoch:322 train_loss:0.2712 val_loss:0.3490 val_accuracy:0.8911\n",
      "epoch:323 train_loss:0.2576 val_loss:0.3503 val_accuracy:0.8878\n",
      "epoch:324 train_loss:0.2658 val_loss:0.3452 val_accuracy:0.8902\n",
      "epoch:325 train_loss:0.2691 val_loss:0.3573 val_accuracy:0.8878\n",
      "epoch:326 train_loss:0.2669 val_loss:0.3506 val_accuracy:0.8906\n",
      "epoch:327 train_loss:0.2635 val_loss:0.3448 val_accuracy:0.8905\n",
      "epoch:328 train_loss:0.2589 val_loss:0.3463 val_accuracy:0.8916\n",
      "epoch:329 train_loss:0.2526 val_loss:0.3418 val_accuracy:0.8939\n",
      "epoch:330 train_loss:0.2579 val_loss:0.3542 val_accuracy:0.8904\n",
      "epoch:331 train_loss:0.2716 val_loss:0.3463 val_accuracy:0.8902\n",
      "epoch:332 train_loss:0.2724 val_loss:0.3435 val_accuracy:0.8907\n",
      "epoch:333 train_loss:0.2628 val_loss:0.3483 val_accuracy:0.8892\n",
      "epoch:334 train_loss:0.2546 val_loss:0.3474 val_accuracy:0.8922\n",
      "epoch:335 train_loss:0.2541 val_loss:0.3425 val_accuracy:0.8916\n",
      "epoch:336 train_loss:0.2588 val_loss:0.3433 val_accuracy:0.8907\n",
      "epoch:337 train_loss:0.2600 val_loss:0.3521 val_accuracy:0.8894\n",
      "epoch:338 train_loss:0.2658 val_loss:0.3683 val_accuracy:0.8831\n",
      "epoch:339 train_loss:0.2539 val_loss:0.3377 val_accuracy:0.8953\n",
      "epoch:340 train_loss:0.2550 val_loss:0.3405 val_accuracy:0.8936\n",
      "epoch:341 train_loss:0.2593 val_loss:0.3508 val_accuracy:0.8904\n",
      "epoch:342 train_loss:0.2567 val_loss:0.3435 val_accuracy:0.8923\n",
      "epoch:343 train_loss:0.2507 val_loss:0.3374 val_accuracy:0.8932\n",
      "epoch:344 train_loss:0.2499 val_loss:0.3384 val_accuracy:0.8935\n",
      "epoch:345 train_loss:0.2565 val_loss:0.3372 val_accuracy:0.8930\n",
      "epoch:346 train_loss:0.2612 val_loss:0.3366 val_accuracy:0.8960\n",
      "epoch:347 train_loss:0.2575 val_loss:0.3396 val_accuracy:0.8907\n",
      "epoch:348 train_loss:0.2537 val_loss:0.3432 val_accuracy:0.8925\n",
      "epoch:349 train_loss:0.2481 val_loss:0.3292 val_accuracy:0.8968\n",
      "epoch:350 train_loss:0.2592 val_loss:0.3337 val_accuracy:0.8962\n",
      "epoch:351 train_loss:0.2404 val_loss:0.3401 val_accuracy:0.8930\n",
      "epoch:352 train_loss:0.2555 val_loss:0.3497 val_accuracy:0.8922\n",
      "epoch:353 train_loss:0.2474 val_loss:0.3396 val_accuracy:0.8928\n",
      "epoch:354 train_loss:0.2514 val_loss:0.3389 val_accuracy:0.8925\n",
      "epoch:355 train_loss:0.2585 val_loss:0.3500 val_accuracy:0.8898\n",
      "epoch:356 train_loss:0.2531 val_loss:0.3479 val_accuracy:0.8909\n",
      "epoch:357 train_loss:0.2463 val_loss:0.3286 val_accuracy:0.8965\n",
      "epoch:358 train_loss:0.2414 val_loss:0.3302 val_accuracy:0.8958\n",
      "epoch:359 train_loss:0.2484 val_loss:0.3356 val_accuracy:0.8948\n",
      "epoch:360 train_loss:0.2537 val_loss:0.3326 val_accuracy:0.8974\n",
      "epoch:361 train_loss:0.2473 val_loss:0.3353 val_accuracy:0.8934\n",
      "epoch:362 train_loss:0.2549 val_loss:0.3412 val_accuracy:0.8906\n",
      "epoch:363 train_loss:0.2564 val_loss:0.3400 val_accuracy:0.8925\n",
      "epoch:364 train_loss:0.2473 val_loss:0.3302 val_accuracy:0.8961\n",
      "epoch:365 train_loss:0.2498 val_loss:0.3377 val_accuracy:0.8923\n",
      "epoch:366 train_loss:0.2479 val_loss:0.3340 val_accuracy:0.8950\n",
      "epoch:367 train_loss:0.2453 val_loss:0.3334 val_accuracy:0.8958\n",
      "epoch:368 train_loss:0.2551 val_loss:0.3350 val_accuracy:0.8924\n",
      "epoch:369 train_loss:0.2557 val_loss:0.3436 val_accuracy:0.8914\n",
      "epoch:370 train_loss:0.2456 val_loss:0.3361 val_accuracy:0.8949\n",
      "epoch:371 train_loss:0.2467 val_loss:0.3255 val_accuracy:0.8956\n",
      "epoch:372 train_loss:0.2469 val_loss:0.3239 val_accuracy:0.8989\n",
      "epoch:373 train_loss:0.2538 val_loss:0.3347 val_accuracy:0.8964\n",
      "epoch:374 train_loss:0.2410 val_loss:0.3405 val_accuracy:0.8923\n",
      "epoch:375 train_loss:0.2477 val_loss:0.3299 val_accuracy:0.8947\n",
      "epoch:376 train_loss:0.2323 val_loss:0.3300 val_accuracy:0.8961\n",
      "epoch:377 train_loss:0.2312 val_loss:0.3237 val_accuracy:0.8984\n",
      "epoch:378 train_loss:0.2345 val_loss:0.3284 val_accuracy:0.8962\n",
      "epoch:379 train_loss:0.2371 val_loss:0.3365 val_accuracy:0.8933\n",
      "epoch:380 train_loss:0.2536 val_loss:0.3429 val_accuracy:0.8904\n",
      "epoch:381 train_loss:0.2372 val_loss:0.3301 val_accuracy:0.8977\n",
      "epoch:382 train_loss:0.2520 val_loss:0.3403 val_accuracy:0.8925\n",
      "epoch:383 train_loss:0.2426 val_loss:0.3277 val_accuracy:0.8961\n",
      "epoch:384 train_loss:0.2414 val_loss:0.3270 val_accuracy:0.8957\n",
      "epoch:385 train_loss:0.2424 val_loss:0.3249 val_accuracy:0.8992\n",
      "epoch:386 train_loss:0.2400 val_loss:0.3285 val_accuracy:0.8959\n",
      "epoch:387 train_loss:0.2466 val_loss:0.3372 val_accuracy:0.8911\n",
      "epoch:388 train_loss:0.2379 val_loss:0.3286 val_accuracy:0.8985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:389 train_loss:0.2312 val_loss:0.3286 val_accuracy:0.8954\n",
      "epoch:390 train_loss:0.2401 val_loss:0.3315 val_accuracy:0.8946\n",
      "epoch:391 train_loss:0.2337 val_loss:0.3218 val_accuracy:0.8991\n",
      "epoch:392 train_loss:0.2419 val_loss:0.3265 val_accuracy:0.8995\n",
      "epoch:393 train_loss:0.2374 val_loss:0.3259 val_accuracy:0.8966\n",
      "epoch:394 train_loss:0.2351 val_loss:0.3235 val_accuracy:0.8973\n",
      "epoch:395 train_loss:0.2386 val_loss:0.3409 val_accuracy:0.8937\n",
      "epoch:396 train_loss:0.2508 val_loss:0.3570 val_accuracy:0.8872\n",
      "epoch:397 train_loss:0.2379 val_loss:0.3209 val_accuracy:0.8986\n",
      "epoch:398 train_loss:0.2330 val_loss:0.3240 val_accuracy:0.8978\n",
      "epoch:399 train_loss:0.2359 val_loss:0.3266 val_accuracy:0.8966\n",
      "epoch:400 train_loss:0.2420 val_loss:0.3268 val_accuracy:0.8956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chainer import iterators\n",
    "from chainer import optimizers\n",
    "from chainer.backends import cuda\n",
    "from chainer.cuda import to_cpu\n",
    "\n",
    "gpu_id = 0 # CPUを用いる場合は、この値を-1にしてください\n",
    "\n",
    "batchsize = 100\n",
    "max_epoch = 400\n",
    "n_mid_units = 200\n",
    "\n",
    "xp = np\n",
    "if gpu_id >= 0:\n",
    "    xp = chainer.cuda.cupy\n",
    "\n",
    "reset_seed(0) # 乱数の種をセット\n",
    "\n",
    "train = np.load(\"MHT-train.npy\") # train: 450 x (framelen x 26 , framelen, maxphonenum)\n",
    "test = np.load(\"MHT-test.npy\")\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "\n",
    "net = RNN(n_lstm_layers=1, n_mid_units=n_mid_units, n_out=nsymbol)\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.1).setup(net)\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    net.to_gpu(gpu_id)\n",
    "    \n",
    "while train_iter.epoch < max_epoch:\n",
    "\n",
    "    # ---------- 学習の1イテレーション ----------\n",
    "    train_batch = train_iter.next()\n",
    "\n",
    "    loss, _ = calculate_loss(net, train_batch, gpu_id, train_mode = True)\n",
    "\n",
    "    # 勾配の計算\n",
    "    net.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    # バッチ単位で古い記憶を削除し、計算コストを削減する。\n",
    "    loss.unchain_backward()\n",
    "\n",
    "    # パラメータの更新\n",
    "    optimizer.update()\n",
    "    \n",
    "    # --------------- ここまで ----------------\n",
    "\n",
    "    # 1エポック終了ごとにValidationデータに対する予測精度を測って、\n",
    "    # モデルの汎化性能が向上していることをチェックしよう\n",
    "    if train_iter.is_new_epoch:  # 1 epochが終わったら\n",
    "        # ロスの表示\n",
    "        print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "            train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "\n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "        \n",
    "        valid_loss, valid_accuracy = calculate_loss(net, test, gpu_id, train_mode=False)\n",
    "        valid_losses.append(to_cpu(valid_loss.array))\n",
    "        valid_accuracies.append(to_cpu(valid_accuracy.array))\n",
    "\n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            np.mean(valid_losses), np.mean(valid_accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フレームごとの音素認識率は90%程度と、feed-forwardに比べて改善していることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 音素認識結果の観察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_utterance_number = 0\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    net.to_gpu(gpu_id)\n",
    "\n",
    "Xs, ts, _ = converter(test, gpu_id)\n",
    "\n",
    "# テストデータを1つ取り出します\n",
    "X_test = Xs[test_utterance_number]\n",
    "t_test = ts[test_utterance_number]\n",
    "\n",
    "with chainer.using_config('train', False), \\\n",
    "     chainer.using_config('enable_backprop', False):\n",
    "    y_test = net([X_test])\n",
    "\n",
    "# Variable形式で出てくるので中身を取り出す\n",
    "y_test = y_test.array\n",
    "\n",
    "# 結果をCPUに送る\n",
    "y_test = to_cpu(y_test)\n",
    "t_test = to_cpu(t_test)\n",
    "\n",
    "# 予測確率の最大値のインデックスを見る\n",
    "pred_label = y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/hiroki/conda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch',\n",
       "       'ch', 'ch', 'ch', 'ch', 'ch', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 's', 's', 's', 's', 's', 's', 's', 's', 's',\n",
       "       's', 'a', 'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'a', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'u', 'u', 'u', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a',\n",
       "       'a', 'a', 'a', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'n',\n",
       "       'n', 'n', 'n', 'n', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e',\n",
       "       'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'k', 'k',\n",
       "       'k', 'k', 'k', 'i', 'i', 'i', 'i', 'i', 'n', 'n', 'n', 'n', 'n',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'y', 'y', 'y', 'y', 'y', 'y', 'y',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o',\n",
       "       'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'o', 'o', 'o', 'o', 'o', 'n', 'n', 'n',\n",
       "       'n', 'n', 'o', 'o', 'o', 'o', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
       "       'g', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'i', 'i', 'i', 'i', 'i', 'i', 'n', 'n',\n",
       "       'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a',\n",
       "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'r', 'r', 'r', 'r', 'r', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'u', 'u', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil'], dtype='<U3')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/hiroki/conda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'ch', 'sil', 'sil', 'sil', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch',\n",
       "       'ch', 'ch', 'ch', 'ch', 'ch', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 's', 's', 's', 's', 's', 's', 's',\n",
       "       's', 's', 'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'n', 'n',\n",
       "       'a', 'a', 'a', 'a', 'a', 'e', 'e', 'u', 'u', 'u', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'n', 'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a',\n",
       "       'a', 'a', 'a', 'e', 'e', 'g', 'g', 'n', 'n', 'g', 'g', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'r',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'r', 'n', 'n', 'n', 'n', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'n',\n",
       "       'g', 'n', 'n', 'd', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e',\n",
       "       'k', 'k', 'k', 'k', 'k', 'Q', 'Q', 'Q', 'Q', 'k', 'k', 'k', 'k',\n",
       "       'k', 'k', 'k', 'i', 'i', 'i', 'i', 'n', 'n', 'n', 'n', 'n', 'n',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'o', 'y', 'y', 'y', 'y', 'y', 'y',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o',\n",
       "       'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'o', 'o', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'o', 'o', 'o', 'o', 'o', 'o', 'n', 'n',\n",
       "       'n', 'n', 'o', 'o', 'o', 'o', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
       "       'n', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'm',\n",
       "       'm', 'm', 'm', 'm', 'n', 'i', 'i', 'i', 'i', 'i', 'i', 'n', 'n',\n",
       "       'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a',\n",
       "       'N', 'N', 'N', 'N', 'N', 'N', 'n', 'n', 'n', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'r', 'r', 'r', 'r', 'r', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'u', 'u', 'u', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil'], dtype='<U3')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前後関係が考慮され、音素が系列として認識されています。記憶を持つネットワークにより、日本語の音素列としてありそうもないものは出力されにくくなっていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
