{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.datasets import mnist\n",
    "from chainer.datasets import TupleDataset\n",
    "from chainer.datasets import split_dataset_random\n",
    "import numpy as np\n",
    "\n",
    "train_load = np.load(\"MHT-train.npy\")\n",
    "train = TupleDataset(train_load[:,0],train_load[:,1])\n",
    "test_load = np.load(\"MHT-test.npy\")\n",
    "test = TupleDataset(test_load[:,0],test_load[:,1])\n",
    "# train: 450 x (framelen x 26 , framelen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import numpy as np\n",
    "\n",
    "gpu_id = 0\n",
    "\n",
    "xp = np\n",
    "if gpu_id >= 0:\n",
    "    xp = chainer.cuda.cupy\n",
    "\n",
    "from chainer import iterators\n",
    "\n",
    "batchsize = 100\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import chainer\n",
    "\n",
    "def reset_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    numpy.random.seed(seed)\n",
    "    if chainer.cuda.available:\n",
    "        chainer.cuda.cupy.random.seed(seed)\n",
    "\n",
    "reset_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "class RNN(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_lstm_layers=1, n_mid_units=100, n_out=40, dropout=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # パラメータを持つ層の登録\n",
    "        with self.init_scope():\n",
    "            # self.l1 = L.Linear(None, n_mid_units)\n",
    "            self.l2 = L.NStepLSTM(n_lstm_layers, 26, n_mid_units, dropout)\n",
    "            self.l3 = L.Linear(n_mid_units, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # データを受け取った際のforward計算を書く\n",
    "        # h1 = F.relu(self.l1(x))\n",
    "        hy, cy, ys = self.l2(None, None, x)\n",
    "        h2 = F.concat(ys, axis=0)\n",
    "        return self.l3(h2)\n",
    "\n",
    "net = RNN(n_lstm_layers=2, n_mid_units=200)\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    net.to_gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.1).setup(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:01 train_loss:3.5112 val_loss:3.4562 val_accuracy:0.2560\n",
      "epoch:02 train_loss:3.3043 val_loss:3.2585 val_accuracy:0.2743\n",
      "epoch:03 train_loss:3.0910 val_loss:3.0419 val_accuracy:0.2913\n",
      "epoch:04 train_loss:2.9280 val_loss:2.8969 val_accuracy:0.3151\n",
      "epoch:05 train_loss:2.7899 val_loss:2.7402 val_accuracy:0.3436\n",
      "epoch:06 train_loss:2.6744 val_loss:2.6290 val_accuracy:0.3616\n",
      "epoch:07 train_loss:2.5263 val_loss:2.5098 val_accuracy:0.3783\n",
      "epoch:08 train_loss:2.4909 val_loss:2.4287 val_accuracy:0.3873\n",
      "epoch:09 train_loss:2.3897 val_loss:2.3445 val_accuracy:0.3969\n",
      "epoch:10 train_loss:2.3499 val_loss:2.2869 val_accuracy:0.4037\n",
      "epoch:11 train_loss:2.2639 val_loss:2.2243 val_accuracy:0.4099\n",
      "epoch:12 train_loss:2.2266 val_loss:2.1807 val_accuracy:0.4155\n",
      "epoch:13 train_loss:2.1619 val_loss:2.1309 val_accuracy:0.4213\n",
      "epoch:14 train_loss:2.1249 val_loss:2.0940 val_accuracy:0.4283\n",
      "epoch:15 train_loss:2.0828 val_loss:2.0513 val_accuracy:0.4365\n",
      "epoch:16 train_loss:2.0386 val_loss:2.0196 val_accuracy:0.4436\n",
      "epoch:17 train_loss:1.9964 val_loss:1.9827 val_accuracy:0.4530\n",
      "epoch:18 train_loss:1.9703 val_loss:1.9540 val_accuracy:0.4597\n",
      "epoch:19 train_loss:1.9255 val_loss:1.9200 val_accuracy:0.4669\n",
      "epoch:20 train_loss:1.9114 val_loss:1.8931 val_accuracy:0.4736\n",
      "epoch:21 train_loss:1.8557 val_loss:1.8616 val_accuracy:0.4815\n",
      "epoch:22 train_loss:1.8694 val_loss:1.8365 val_accuracy:0.4883\n",
      "epoch:23 train_loss:1.8178 val_loss:1.8071 val_accuracy:0.4946\n",
      "epoch:24 train_loss:1.7920 val_loss:1.7843 val_accuracy:0.5011\n",
      "epoch:25 train_loss:1.7831 val_loss:1.7555 val_accuracy:0.5132\n",
      "epoch:26 train_loss:1.7838 val_loss:1.7343 val_accuracy:0.5229\n",
      "epoch:27 train_loss:1.7083 val_loss:1.7085 val_accuracy:0.5271\n",
      "epoch:28 train_loss:1.6947 val_loss:1.6874 val_accuracy:0.5407\n",
      "epoch:29 train_loss:1.6917 val_loss:1.6624 val_accuracy:0.5589\n",
      "epoch:30 train_loss:1.6645 val_loss:1.6415 val_accuracy:0.5653\n",
      "epoch:31 train_loss:1.6163 val_loss:1.6180 val_accuracy:0.5637\n",
      "epoch:32 train_loss:1.6296 val_loss:1.5960 val_accuracy:0.5859\n",
      "epoch:33 train_loss:1.5798 val_loss:1.5717 val_accuracy:0.5926\n",
      "epoch:34 train_loss:1.5734 val_loss:1.5579 val_accuracy:0.5851\n",
      "epoch:35 train_loss:1.5416 val_loss:1.5311 val_accuracy:0.5901\n",
      "epoch:36 train_loss:1.5291 val_loss:1.5054 val_accuracy:0.6078\n",
      "epoch:37 train_loss:1.5443 val_loss:1.4893 val_accuracy:0.6114\n",
      "epoch:38 train_loss:1.5193 val_loss:1.4679 val_accuracy:0.6176\n",
      "epoch:39 train_loss:1.4507 val_loss:1.4675 val_accuracy:0.5968\n",
      "epoch:40 train_loss:1.4663 val_loss:1.4403 val_accuracy:0.6195\n",
      "epoch:41 train_loss:1.4200 val_loss:1.4043 val_accuracy:0.6298\n",
      "epoch:42 train_loss:1.4064 val_loss:1.3884 val_accuracy:0.6339\n",
      "epoch:43 train_loss:1.4031 val_loss:1.4652 val_accuracy:0.5792\n",
      "epoch:44 train_loss:1.3684 val_loss:1.3510 val_accuracy:0.6400\n",
      "epoch:45 train_loss:1.4494 val_loss:1.3565 val_accuracy:0.6292\n",
      "epoch:46 train_loss:1.3426 val_loss:1.3205 val_accuracy:0.6489\n",
      "epoch:47 train_loss:1.3497 val_loss:1.3960 val_accuracy:0.6037\n",
      "epoch:48 train_loss:1.3218 val_loss:1.3047 val_accuracy:0.6416\n",
      "epoch:49 train_loss:1.2886 val_loss:1.2863 val_accuracy:0.6499\n",
      "epoch:50 train_loss:1.3414 val_loss:1.2664 val_accuracy:0.6579\n",
      "epoch:51 train_loss:1.2643 val_loss:1.2585 val_accuracy:0.6559\n",
      "epoch:52 train_loss:1.3200 val_loss:1.2875 val_accuracy:0.6348\n",
      "epoch:53 train_loss:1.2414 val_loss:1.2338 val_accuracy:0.6604\n",
      "epoch:54 train_loss:1.2672 val_loss:1.2568 val_accuracy:0.6470\n",
      "epoch:55 train_loss:1.2466 val_loss:1.2083 val_accuracy:0.6682\n",
      "epoch:56 train_loss:1.2567 val_loss:1.1996 val_accuracy:0.6715\n",
      "epoch:57 train_loss:1.2175 val_loss:1.1832 val_accuracy:0.6742\n",
      "epoch:58 train_loss:1.2074 val_loss:1.1802 val_accuracy:0.6758\n",
      "epoch:59 train_loss:1.2164 val_loss:1.1772 val_accuracy:0.6711\n",
      "epoch:60 train_loss:1.1964 val_loss:1.1801 val_accuracy:0.6653\n",
      "epoch:61 train_loss:1.1617 val_loss:1.1539 val_accuracy:0.6806\n",
      "epoch:62 train_loss:1.2133 val_loss:1.1486 val_accuracy:0.6867\n",
      "epoch:63 train_loss:1.1287 val_loss:1.1412 val_accuracy:0.6784\n",
      "epoch:64 train_loss:1.1739 val_loss:1.1241 val_accuracy:0.6849\n",
      "epoch:65 train_loss:1.1390 val_loss:1.1158 val_accuracy:0.6910\n",
      "epoch:66 train_loss:1.1703 val_loss:1.1090 val_accuracy:0.6921\n",
      "epoch:67 train_loss:1.1343 val_loss:1.1012 val_accuracy:0.6886\n",
      "epoch:68 train_loss:1.1045 val_loss:1.1047 val_accuracy:0.6867\n",
      "epoch:69 train_loss:1.1812 val_loss:1.0937 val_accuracy:0.6940\n",
      "epoch:70 train_loss:1.0927 val_loss:1.0752 val_accuracy:0.6955\n",
      "epoch:71 train_loss:1.0727 val_loss:1.0691 val_accuracy:0.6953\n",
      "epoch:72 train_loss:1.1061 val_loss:1.2112 val_accuracy:0.6365\n",
      "epoch:73 train_loss:1.0712 val_loss:1.0534 val_accuracy:0.7023\n",
      "epoch:74 train_loss:1.0680 val_loss:1.0461 val_accuracy:0.7033\n",
      "epoch:75 train_loss:1.0607 val_loss:1.0417 val_accuracy:0.7027\n",
      "epoch:76 train_loss:1.0673 val_loss:1.0357 val_accuracy:0.7045\n",
      "epoch:77 train_loss:1.0386 val_loss:1.0256 val_accuracy:0.7086\n",
      "epoch:78 train_loss:1.0935 val_loss:1.0467 val_accuracy:0.6994\n",
      "epoch:79 train_loss:1.0368 val_loss:1.0293 val_accuracy:0.7030\n",
      "epoch:80 train_loss:1.0327 val_loss:1.0107 val_accuracy:0.7108\n",
      "epoch:81 train_loss:1.0484 val_loss:1.0055 val_accuracy:0.7166\n",
      "epoch:82 train_loss:0.9972 val_loss:0.9967 val_accuracy:0.7160\n",
      "epoch:83 train_loss:1.0289 val_loss:0.9942 val_accuracy:0.7138\n",
      "epoch:84 train_loss:1.0201 val_loss:0.9838 val_accuracy:0.7184\n",
      "epoch:85 train_loss:1.0093 val_loss:1.0054 val_accuracy:0.7062\n",
      "epoch:86 train_loss:1.0079 val_loss:0.9903 val_accuracy:0.7118\n",
      "epoch:87 train_loss:0.9909 val_loss:0.9701 val_accuracy:0.7237\n",
      "epoch:88 train_loss:1.0049 val_loss:0.9634 val_accuracy:0.7248\n",
      "epoch:89 train_loss:0.9962 val_loss:0.9601 val_accuracy:0.7241\n",
      "epoch:90 train_loss:0.9608 val_loss:0.9558 val_accuracy:0.7261\n",
      "epoch:91 train_loss:1.0013 val_loss:1.0105 val_accuracy:0.7022\n",
      "epoch:92 train_loss:0.9977 val_loss:0.9919 val_accuracy:0.7055\n",
      "epoch:93 train_loss:0.9559 val_loss:0.9416 val_accuracy:0.7300\n",
      "epoch:94 train_loss:0.9599 val_loss:0.9384 val_accuracy:0.7286\n",
      "epoch:95 train_loss:0.9610 val_loss:0.9456 val_accuracy:0.7249\n",
      "epoch:96 train_loss:0.9597 val_loss:0.9479 val_accuracy:0.7234\n",
      "epoch:97 train_loss:0.9467 val_loss:0.9271 val_accuracy:0.7359\n",
      "epoch:98 train_loss:0.9448 val_loss:0.9193 val_accuracy:0.7371\n",
      "epoch:99 train_loss:0.9539 val_loss:0.9474 val_accuracy:0.7231\n",
      "epoch:100 train_loss:0.9198 val_loss:0.9157 val_accuracy:0.7352\n",
      "epoch:101 train_loss:0.9277 val_loss:0.9083 val_accuracy:0.7374\n",
      "epoch:102 train_loss:0.9218 val_loss:0.9048 val_accuracy:0.7387\n",
      "epoch:103 train_loss:0.9201 val_loss:0.9030 val_accuracy:0.7409\n",
      "epoch:104 train_loss:0.9175 val_loss:0.8974 val_accuracy:0.7426\n",
      "epoch:105 train_loss:0.9025 val_loss:0.8979 val_accuracy:0.7393\n",
      "epoch:106 train_loss:0.8952 val_loss:0.8930 val_accuracy:0.7451\n",
      "epoch:107 train_loss:0.9242 val_loss:0.9362 val_accuracy:0.7208\n",
      "epoch:108 train_loss:0.9040 val_loss:0.9106 val_accuracy:0.7328\n",
      "epoch:109 train_loss:0.8843 val_loss:0.8822 val_accuracy:0.7448\n",
      "epoch:110 train_loss:0.8703 val_loss:0.8795 val_accuracy:0.7457\n",
      "epoch:111 train_loss:0.8968 val_loss:0.8809 val_accuracy:0.7450\n",
      "epoch:112 train_loss:0.8569 val_loss:0.8715 val_accuracy:0.7490\n",
      "epoch:113 train_loss:0.8458 val_loss:0.8684 val_accuracy:0.7491\n",
      "epoch:114 train_loss:0.8716 val_loss:0.8644 val_accuracy:0.7521\n",
      "epoch:115 train_loss:0.8657 val_loss:0.8655 val_accuracy:0.7500\n",
      "epoch:116 train_loss:0.8629 val_loss:0.8668 val_accuracy:0.7489\n",
      "epoch:117 train_loss:0.8695 val_loss:0.8556 val_accuracy:0.7529\n",
      "epoch:118 train_loss:0.8548 val_loss:0.8509 val_accuracy:0.7565\n",
      "epoch:119 train_loss:0.8386 val_loss:0.8572 val_accuracy:0.7511\n",
      "epoch:120 train_loss:0.8757 val_loss:0.9100 val_accuracy:0.7326\n",
      "epoch:121 train_loss:0.9224 val_loss:0.8755 val_accuracy:0.7529\n",
      "epoch:122 train_loss:0.8409 val_loss:0.8480 val_accuracy:0.7544\n",
      "epoch:123 train_loss:0.8396 val_loss:0.8396 val_accuracy:0.7581\n",
      "epoch:124 train_loss:0.8338 val_loss:0.8347 val_accuracy:0.7604\n",
      "epoch:125 train_loss:0.8373 val_loss:0.8332 val_accuracy:0.7606\n",
      "epoch:126 train_loss:0.8332 val_loss:0.8307 val_accuracy:0.7601\n",
      "epoch:127 train_loss:0.8239 val_loss:0.8263 val_accuracy:0.7615\n",
      "epoch:128 train_loss:0.8160 val_loss:0.8241 val_accuracy:0.7635\n",
      "epoch:129 train_loss:0.8356 val_loss:0.8269 val_accuracy:0.7607\n",
      "epoch:130 train_loss:0.8291 val_loss:0.8275 val_accuracy:0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:131 train_loss:0.8320 val_loss:0.8195 val_accuracy:0.7639\n",
      "epoch:132 train_loss:0.8222 val_loss:0.8139 val_accuracy:0.7658\n",
      "epoch:133 train_loss:0.8061 val_loss:0.8106 val_accuracy:0.7662\n",
      "epoch:134 train_loss:0.7943 val_loss:0.8120 val_accuracy:0.7658\n",
      "epoch:135 train_loss:0.8095 val_loss:0.8077 val_accuracy:0.7672\n",
      "epoch:136 train_loss:0.8031 val_loss:0.8409 val_accuracy:0.7599\n",
      "epoch:137 train_loss:0.8953 val_loss:1.0858 val_accuracy:0.6677\n",
      "epoch:138 train_loss:0.8127 val_loss:0.8218 val_accuracy:0.7639\n",
      "epoch:139 train_loss:0.7888 val_loss:0.8055 val_accuracy:0.7687\n",
      "epoch:140 train_loss:0.8016 val_loss:0.8016 val_accuracy:0.7698\n",
      "epoch:141 train_loss:0.7994 val_loss:0.7982 val_accuracy:0.7705\n",
      "epoch:142 train_loss:0.7610 val_loss:0.7951 val_accuracy:0.7709\n",
      "epoch:143 train_loss:0.7854 val_loss:0.7913 val_accuracy:0.7731\n",
      "epoch:144 train_loss:0.7905 val_loss:0.7928 val_accuracy:0.7716\n",
      "epoch:145 train_loss:0.7721 val_loss:0.7928 val_accuracy:0.7715\n",
      "epoch:146 train_loss:0.7645 val_loss:0.7892 val_accuracy:0.7728\n",
      "epoch:147 train_loss:0.7735 val_loss:0.7866 val_accuracy:0.7732\n",
      "epoch:148 train_loss:0.7799 val_loss:0.7868 val_accuracy:0.7723\n",
      "epoch:149 train_loss:0.7569 val_loss:0.7807 val_accuracy:0.7749\n",
      "epoch:150 train_loss:0.7475 val_loss:0.7797 val_accuracy:0.7741\n",
      "epoch:151 train_loss:0.7521 val_loss:0.7763 val_accuracy:0.7760\n",
      "epoch:152 train_loss:0.7597 val_loss:0.7719 val_accuracy:0.7780\n",
      "epoch:153 train_loss:0.7490 val_loss:0.7789 val_accuracy:0.7739\n",
      "epoch:154 train_loss:0.7605 val_loss:0.7752 val_accuracy:0.7777\n",
      "epoch:155 train_loss:0.7271 val_loss:0.7706 val_accuracy:0.7760\n",
      "epoch:156 train_loss:0.7509 val_loss:0.7670 val_accuracy:0.7770\n",
      "epoch:157 train_loss:0.7372 val_loss:0.7714 val_accuracy:0.7761\n",
      "epoch:158 train_loss:0.7431 val_loss:0.7668 val_accuracy:0.7780\n",
      "epoch:159 train_loss:0.7211 val_loss:0.7607 val_accuracy:0.7800\n",
      "epoch:160 train_loss:0.7553 val_loss:0.7588 val_accuracy:0.7814\n",
      "epoch:161 train_loss:0.7308 val_loss:0.7530 val_accuracy:0.7825\n",
      "epoch:162 train_loss:0.7440 val_loss:0.7584 val_accuracy:0.7796\n",
      "epoch:163 train_loss:0.7418 val_loss:0.7554 val_accuracy:0.7829\n",
      "epoch:164 train_loss:0.7175 val_loss:0.7543 val_accuracy:0.7808\n",
      "epoch:165 train_loss:0.7163 val_loss:0.7473 val_accuracy:0.7845\n",
      "epoch:166 train_loss:0.7312 val_loss:0.7453 val_accuracy:0.7846\n",
      "epoch:167 train_loss:0.7242 val_loss:0.7430 val_accuracy:0.7851\n",
      "epoch:168 train_loss:0.7137 val_loss:0.7415 val_accuracy:0.7854\n",
      "epoch:169 train_loss:0.7264 val_loss:0.7427 val_accuracy:0.7846\n",
      "epoch:170 train_loss:0.7172 val_loss:0.7394 val_accuracy:0.7875\n",
      "epoch:171 train_loss:0.7618 val_loss:1.0899 val_accuracy:0.6912\n",
      "epoch:172 train_loss:0.7264 val_loss:0.7416 val_accuracy:0.7866\n",
      "epoch:173 train_loss:0.7159 val_loss:0.7346 val_accuracy:0.7875\n",
      "epoch:174 train_loss:0.7067 val_loss:0.7294 val_accuracy:0.7891\n",
      "epoch:175 train_loss:0.7059 val_loss:0.7258 val_accuracy:0.7905\n",
      "epoch:176 train_loss:0.6914 val_loss:0.7296 val_accuracy:0.7879\n",
      "epoch:177 train_loss:0.6877 val_loss:0.7264 val_accuracy:0.7897\n",
      "epoch:178 train_loss:0.6901 val_loss:0.7236 val_accuracy:0.7906\n",
      "epoch:179 train_loss:0.7002 val_loss:0.7297 val_accuracy:0.7880\n",
      "epoch:180 train_loss:0.6844 val_loss:0.7241 val_accuracy:0.7906\n",
      "epoch:181 train_loss:0.6841 val_loss:0.7178 val_accuracy:0.7923\n",
      "epoch:182 train_loss:0.6992 val_loss:0.7173 val_accuracy:0.7915\n",
      "epoch:183 train_loss:0.6791 val_loss:0.7178 val_accuracy:0.7929\n",
      "epoch:184 train_loss:0.6724 val_loss:0.7204 val_accuracy:0.7904\n",
      "epoch:185 train_loss:0.6884 val_loss:0.7129 val_accuracy:0.7940\n",
      "epoch:186 train_loss:0.6984 val_loss:0.7191 val_accuracy:0.7903\n",
      "epoch:187 train_loss:0.7001 val_loss:0.7092 val_accuracy:0.7931\n",
      "epoch:188 train_loss:0.6726 val_loss:0.7120 val_accuracy:0.7929\n",
      "epoch:189 train_loss:0.6675 val_loss:0.7159 val_accuracy:0.7917\n",
      "epoch:190 train_loss:0.6815 val_loss:0.7083 val_accuracy:0.7938\n",
      "epoch:191 train_loss:0.6696 val_loss:0.7051 val_accuracy:0.7961\n",
      "epoch:192 train_loss:0.6767 val_loss:0.7176 val_accuracy:0.7911\n",
      "epoch:193 train_loss:0.7045 val_loss:0.7230 val_accuracy:0.7878\n",
      "epoch:194 train_loss:0.6409 val_loss:0.6993 val_accuracy:0.7955\n",
      "epoch:195 train_loss:0.6675 val_loss:0.7001 val_accuracy:0.7961\n",
      "epoch:196 train_loss:0.6681 val_loss:0.7046 val_accuracy:0.7948\n",
      "epoch:197 train_loss:0.6576 val_loss:0.6971 val_accuracy:0.7968\n",
      "epoch:198 train_loss:0.6606 val_loss:0.6964 val_accuracy:0.7977\n",
      "epoch:199 train_loss:0.6811 val_loss:0.6983 val_accuracy:0.7974\n",
      "epoch:200 train_loss:0.6661 val_loss:0.6988 val_accuracy:0.7959\n",
      "epoch:201 train_loss:0.6694 val_loss:0.6964 val_accuracy:0.7975\n",
      "epoch:202 train_loss:0.6488 val_loss:0.6960 val_accuracy:0.7976\n",
      "epoch:203 train_loss:0.6518 val_loss:0.6914 val_accuracy:0.7981\n",
      "epoch:204 train_loss:0.6587 val_loss:0.6985 val_accuracy:0.7945\n",
      "epoch:205 train_loss:0.6358 val_loss:0.6893 val_accuracy:0.7988\n",
      "epoch:206 train_loss:0.6555 val_loss:0.6963 val_accuracy:0.7965\n",
      "epoch:207 train_loss:0.6405 val_loss:0.6907 val_accuracy:0.7972\n",
      "epoch:208 train_loss:0.6327 val_loss:0.6861 val_accuracy:0.7981\n",
      "epoch:209 train_loss:0.6301 val_loss:0.6931 val_accuracy:0.7972\n",
      "epoch:210 train_loss:0.6304 val_loss:0.6818 val_accuracy:0.8001\n",
      "epoch:211 train_loss:0.6409 val_loss:0.6845 val_accuracy:0.7983\n",
      "epoch:212 train_loss:0.6285 val_loss:0.6944 val_accuracy:0.7956\n",
      "epoch:213 train_loss:0.6277 val_loss:0.6830 val_accuracy:0.7992\n",
      "epoch:214 train_loss:0.6383 val_loss:0.6859 val_accuracy:0.7982\n",
      "epoch:215 train_loss:0.6110 val_loss:0.6773 val_accuracy:0.8031\n",
      "epoch:216 train_loss:0.6397 val_loss:0.6728 val_accuracy:0.8022\n",
      "epoch:217 train_loss:0.6237 val_loss:0.6870 val_accuracy:0.7996\n",
      "epoch:218 train_loss:0.6389 val_loss:0.6850 val_accuracy:0.7991\n",
      "epoch:219 train_loss:0.6383 val_loss:0.6790 val_accuracy:0.7995\n",
      "epoch:220 train_loss:0.6288 val_loss:0.6785 val_accuracy:0.8002\n",
      "epoch:221 train_loss:0.6248 val_loss:0.6711 val_accuracy:0.8010\n",
      "epoch:222 train_loss:0.6052 val_loss:0.6679 val_accuracy:0.8036\n",
      "epoch:223 train_loss:0.6135 val_loss:0.6711 val_accuracy:0.8022\n",
      "epoch:224 train_loss:0.6027 val_loss:0.6679 val_accuracy:0.8044\n",
      "epoch:225 train_loss:0.6287 val_loss:0.6729 val_accuracy:0.8027\n",
      "epoch:226 train_loss:0.6917 val_loss:0.8513 val_accuracy:0.7418\n",
      "epoch:227 train_loss:0.8412 val_loss:0.9109 val_accuracy:0.7109\n",
      "epoch:228 train_loss:0.7181 val_loss:0.7201 val_accuracy:0.7858\n",
      "epoch:229 train_loss:0.6734 val_loss:0.6874 val_accuracy:0.7975\n",
      "epoch:230 train_loss:0.6377 val_loss:0.6728 val_accuracy:0.8025\n",
      "epoch:231 train_loss:0.6229 val_loss:0.6638 val_accuracy:0.8043\n",
      "epoch:232 train_loss:0.6159 val_loss:0.6585 val_accuracy:0.8047\n",
      "epoch:233 train_loss:0.6065 val_loss:0.6580 val_accuracy:0.8050\n",
      "epoch:234 train_loss:0.6120 val_loss:0.6572 val_accuracy:0.8041\n",
      "epoch:235 train_loss:0.6252 val_loss:0.6549 val_accuracy:0.8071\n",
      "epoch:236 train_loss:0.5976 val_loss:0.6508 val_accuracy:0.8085\n",
      "epoch:237 train_loss:0.6149 val_loss:0.6475 val_accuracy:0.8100\n",
      "epoch:238 train_loss:0.5837 val_loss:0.6486 val_accuracy:0.8094\n",
      "epoch:239 train_loss:0.6240 val_loss:0.6492 val_accuracy:0.8091\n",
      "epoch:240 train_loss:0.5985 val_loss:0.6473 val_accuracy:0.8108\n",
      "epoch:241 train_loss:0.5934 val_loss:0.6430 val_accuracy:0.8123\n",
      "epoch:242 train_loss:0.6173 val_loss:0.6501 val_accuracy:0.8094\n",
      "epoch:243 train_loss:0.6069 val_loss:0.6416 val_accuracy:0.8110\n",
      "epoch:244 train_loss:0.5932 val_loss:0.6440 val_accuracy:0.8109\n",
      "epoch:245 train_loss:0.5920 val_loss:0.6423 val_accuracy:0.8123\n",
      "epoch:246 train_loss:0.5864 val_loss:0.6422 val_accuracy:0.8112\n",
      "epoch:247 train_loss:0.5858 val_loss:0.6376 val_accuracy:0.8140\n",
      "epoch:248 train_loss:0.5964 val_loss:0.6375 val_accuracy:0.8122\n",
      "epoch:249 train_loss:0.5507 val_loss:0.6365 val_accuracy:0.8142\n",
      "epoch:250 train_loss:0.5789 val_loss:0.6373 val_accuracy:0.8139\n",
      "epoch:251 train_loss:0.5925 val_loss:0.6341 val_accuracy:0.8145\n",
      "epoch:252 train_loss:0.5770 val_loss:0.6359 val_accuracy:0.8131\n",
      "epoch:253 train_loss:0.5954 val_loss:0.6319 val_accuracy:0.8145\n",
      "epoch:254 train_loss:0.5841 val_loss:0.6329 val_accuracy:0.8139\n",
      "epoch:255 train_loss:0.5608 val_loss:0.6290 val_accuracy:0.8159\n",
      "epoch:256 train_loss:0.5631 val_loss:0.6286 val_accuracy:0.8170\n",
      "epoch:257 train_loss:0.5861 val_loss:0.6334 val_accuracy:0.8149\n",
      "epoch:258 train_loss:0.5766 val_loss:0.6297 val_accuracy:0.8148\n",
      "epoch:259 train_loss:0.5882 val_loss:0.6240 val_accuracy:0.8185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:260 train_loss:0.5569 val_loss:0.6273 val_accuracy:0.8167\n",
      "epoch:261 train_loss:0.5681 val_loss:0.6277 val_accuracy:0.8168\n",
      "epoch:262 train_loss:0.5577 val_loss:0.6276 val_accuracy:0.8167\n",
      "epoch:263 train_loss:0.5613 val_loss:0.6260 val_accuracy:0.8162\n",
      "epoch:264 train_loss:0.5813 val_loss:0.6456 val_accuracy:0.8103\n",
      "epoch:265 train_loss:0.6893 val_loss:0.9219 val_accuracy:0.7307\n",
      "epoch:266 train_loss:0.5696 val_loss:0.6227 val_accuracy:0.8150\n",
      "epoch:267 train_loss:0.5746 val_loss:0.6150 val_accuracy:0.8183\n",
      "epoch:268 train_loss:0.5689 val_loss:0.6149 val_accuracy:0.8186\n",
      "epoch:269 train_loss:0.5603 val_loss:0.6145 val_accuracy:0.8186\n",
      "epoch:270 train_loss:0.5584 val_loss:0.6139 val_accuracy:0.8196\n",
      "epoch:271 train_loss:0.5380 val_loss:0.6123 val_accuracy:0.8199\n",
      "epoch:272 train_loss:0.5638 val_loss:0.6141 val_accuracy:0.8204\n",
      "epoch:273 train_loss:0.5621 val_loss:0.6146 val_accuracy:0.8208\n",
      "epoch:274 train_loss:0.5545 val_loss:0.6114 val_accuracy:0.8190\n",
      "epoch:275 train_loss:0.5551 val_loss:0.6107 val_accuracy:0.8206\n",
      "epoch:276 train_loss:0.5590 val_loss:0.6109 val_accuracy:0.8198\n",
      "epoch:277 train_loss:0.5674 val_loss:0.6154 val_accuracy:0.8200\n",
      "epoch:278 train_loss:0.5372 val_loss:0.6111 val_accuracy:0.8201\n",
      "epoch:279 train_loss:0.5368 val_loss:0.6105 val_accuracy:0.8214\n",
      "epoch:280 train_loss:0.5450 val_loss:0.6079 val_accuracy:0.8210\n",
      "epoch:281 train_loss:0.5433 val_loss:0.6075 val_accuracy:0.8218\n",
      "epoch:282 train_loss:0.5241 val_loss:0.6142 val_accuracy:0.8200\n",
      "epoch:283 train_loss:0.5393 val_loss:0.6073 val_accuracy:0.8211\n",
      "epoch:284 train_loss:0.5368 val_loss:0.6082 val_accuracy:0.8202\n",
      "epoch:285 train_loss:0.5412 val_loss:0.6016 val_accuracy:0.8227\n",
      "epoch:286 train_loss:0.5506 val_loss:0.6280 val_accuracy:0.8148\n",
      "epoch:287 train_loss:0.5413 val_loss:0.6006 val_accuracy:0.8227\n",
      "epoch:288 train_loss:0.5406 val_loss:0.5987 val_accuracy:0.8235\n",
      "epoch:289 train_loss:0.5238 val_loss:0.5993 val_accuracy:0.8236\n",
      "epoch:290 train_loss:0.5513 val_loss:0.6010 val_accuracy:0.8216\n",
      "epoch:291 train_loss:0.5228 val_loss:0.6002 val_accuracy:0.8217\n",
      "epoch:292 train_loss:0.5328 val_loss:0.5996 val_accuracy:0.8236\n",
      "epoch:293 train_loss:0.5263 val_loss:0.6011 val_accuracy:0.8224\n",
      "epoch:294 train_loss:0.5383 val_loss:0.6015 val_accuracy:0.8228\n",
      "epoch:295 train_loss:0.5349 val_loss:0.5977 val_accuracy:0.8237\n",
      "epoch:296 train_loss:0.5250 val_loss:0.5996 val_accuracy:0.8238\n",
      "epoch:297 train_loss:0.5277 val_loss:0.5993 val_accuracy:0.8235\n",
      "epoch:298 train_loss:0.5356 val_loss:0.5964 val_accuracy:0.8240\n",
      "epoch:299 train_loss:0.5378 val_loss:0.5936 val_accuracy:0.8239\n",
      "epoch:300 train_loss:0.5224 val_loss:0.5917 val_accuracy:0.8244\n",
      "epoch:301 train_loss:0.5127 val_loss:0.5922 val_accuracy:0.8251\n",
      "epoch:302 train_loss:0.5221 val_loss:0.5948 val_accuracy:0.8219\n",
      "epoch:303 train_loss:0.5228 val_loss:0.5872 val_accuracy:0.8251\n",
      "epoch:304 train_loss:0.5262 val_loss:0.5879 val_accuracy:0.8253\n",
      "epoch:305 train_loss:0.5166 val_loss:0.5900 val_accuracy:0.8254\n",
      "epoch:306 train_loss:0.5302 val_loss:0.5899 val_accuracy:0.8261\n",
      "epoch:307 train_loss:0.5089 val_loss:0.5924 val_accuracy:0.8251\n",
      "epoch:308 train_loss:0.5294 val_loss:0.5865 val_accuracy:0.8257\n",
      "epoch:309 train_loss:0.5094 val_loss:0.5912 val_accuracy:0.8258\n",
      "epoch:310 train_loss:0.5100 val_loss:0.5865 val_accuracy:0.8263\n",
      "epoch:311 train_loss:0.5005 val_loss:0.5877 val_accuracy:0.8264\n",
      "epoch:312 train_loss:0.5171 val_loss:0.5846 val_accuracy:0.8262\n",
      "epoch:313 train_loss:0.4908 val_loss:0.5849 val_accuracy:0.8266\n",
      "epoch:314 train_loss:0.5108 val_loss:0.5965 val_accuracy:0.8241\n",
      "epoch:315 train_loss:0.5025 val_loss:0.5839 val_accuracy:0.8261\n",
      "epoch:316 train_loss:0.4945 val_loss:0.5903 val_accuracy:0.8244\n",
      "epoch:317 train_loss:0.5098 val_loss:0.5843 val_accuracy:0.8264\n",
      "epoch:318 train_loss:0.4883 val_loss:0.5847 val_accuracy:0.8274\n",
      "epoch:319 train_loss:0.5109 val_loss:0.5810 val_accuracy:0.8272\n",
      "epoch:320 train_loss:0.5067 val_loss:0.5835 val_accuracy:0.8273\n",
      "epoch:321 train_loss:0.4981 val_loss:0.6024 val_accuracy:0.8209\n",
      "epoch:322 train_loss:0.6000 val_loss:0.8245 val_accuracy:0.7621\n",
      "epoch:323 train_loss:0.5871 val_loss:0.6182 val_accuracy:0.8185\n",
      "epoch:324 train_loss:0.5415 val_loss:0.5881 val_accuracy:0.8244\n",
      "epoch:325 train_loss:0.5184 val_loss:0.5830 val_accuracy:0.8240\n",
      "epoch:326 train_loss:0.5181 val_loss:0.5771 val_accuracy:0.8252\n",
      "epoch:327 train_loss:0.5011 val_loss:0.5764 val_accuracy:0.8269\n",
      "epoch:328 train_loss:0.5397 val_loss:0.5771 val_accuracy:0.8275\n",
      "epoch:329 train_loss:0.5051 val_loss:0.5699 val_accuracy:0.8287\n",
      "epoch:330 train_loss:0.5022 val_loss:0.5689 val_accuracy:0.8303\n",
      "epoch:331 train_loss:0.4985 val_loss:0.5706 val_accuracy:0.8298\n",
      "epoch:332 train_loss:0.5089 val_loss:0.5669 val_accuracy:0.8310\n",
      "epoch:333 train_loss:0.5026 val_loss:0.5669 val_accuracy:0.8318\n",
      "epoch:334 train_loss:0.4907 val_loss:0.5658 val_accuracy:0.8322\n",
      "epoch:335 train_loss:0.5055 val_loss:0.5661 val_accuracy:0.8316\n",
      "epoch:336 train_loss:0.4842 val_loss:0.5673 val_accuracy:0.8318\n",
      "epoch:337 train_loss:0.5005 val_loss:0.5633 val_accuracy:0.8320\n",
      "epoch:338 train_loss:0.5022 val_loss:0.5702 val_accuracy:0.8296\n",
      "epoch:339 train_loss:0.5067 val_loss:0.5665 val_accuracy:0.8307\n",
      "epoch:340 train_loss:0.4908 val_loss:0.5635 val_accuracy:0.8322\n",
      "epoch:341 train_loss:0.5046 val_loss:0.5677 val_accuracy:0.8316\n",
      "epoch:342 train_loss:0.4917 val_loss:0.5649 val_accuracy:0.8315\n",
      "epoch:343 train_loss:0.5466 val_loss:0.5763 val_accuracy:0.8258\n",
      "epoch:344 train_loss:0.4825 val_loss:0.5615 val_accuracy:0.8315\n",
      "epoch:345 train_loss:0.4739 val_loss:0.5592 val_accuracy:0.8332\n",
      "epoch:346 train_loss:0.4832 val_loss:0.5637 val_accuracy:0.8313\n",
      "epoch:347 train_loss:0.4772 val_loss:0.5621 val_accuracy:0.8326\n",
      "epoch:348 train_loss:0.4876 val_loss:0.5624 val_accuracy:0.8324\n",
      "epoch:349 train_loss:0.4846 val_loss:0.5618 val_accuracy:0.8319\n",
      "epoch:350 train_loss:0.4788 val_loss:0.5612 val_accuracy:0.8319\n",
      "epoch:351 train_loss:0.4635 val_loss:0.5610 val_accuracy:0.8326\n",
      "epoch:352 train_loss:0.4955 val_loss:0.5546 val_accuracy:0.8343\n",
      "epoch:353 train_loss:0.4943 val_loss:0.5574 val_accuracy:0.8336\n",
      "epoch:354 train_loss:0.4854 val_loss:0.5594 val_accuracy:0.8324\n",
      "epoch:355 train_loss:0.4826 val_loss:0.5557 val_accuracy:0.8333\n",
      "epoch:356 train_loss:0.4833 val_loss:0.5583 val_accuracy:0.8337\n",
      "epoch:357 train_loss:0.4701 val_loss:0.5551 val_accuracy:0.8345\n",
      "epoch:358 train_loss:0.4749 val_loss:0.5531 val_accuracy:0.8340\n",
      "epoch:359 train_loss:0.4705 val_loss:0.5554 val_accuracy:0.8339\n",
      "epoch:360 train_loss:0.4663 val_loss:0.5627 val_accuracy:0.8328\n",
      "epoch:361 train_loss:0.4733 val_loss:0.5544 val_accuracy:0.8338\n",
      "epoch:362 train_loss:0.4838 val_loss:0.5536 val_accuracy:0.8345\n",
      "epoch:363 train_loss:0.4621 val_loss:0.5547 val_accuracy:0.8341\n",
      "epoch:364 train_loss:0.4805 val_loss:0.5542 val_accuracy:0.8350\n",
      "epoch:365 train_loss:0.4702 val_loss:0.5528 val_accuracy:0.8348\n",
      "epoch:366 train_loss:0.4526 val_loss:0.5532 val_accuracy:0.8347\n",
      "epoch:367 train_loss:0.4590 val_loss:0.5549 val_accuracy:0.8343\n",
      "epoch:368 train_loss:0.4648 val_loss:0.5584 val_accuracy:0.8317\n",
      "epoch:369 train_loss:0.4700 val_loss:0.5566 val_accuracy:0.8340\n",
      "epoch:370 train_loss:0.4652 val_loss:0.5507 val_accuracy:0.8356\n",
      "epoch:371 train_loss:0.4676 val_loss:0.5477 val_accuracy:0.8367\n",
      "epoch:372 train_loss:0.4657 val_loss:0.5551 val_accuracy:0.8341\n",
      "epoch:373 train_loss:0.4749 val_loss:0.5511 val_accuracy:0.8355\n",
      "epoch:374 train_loss:0.4815 val_loss:0.5552 val_accuracy:0.8344\n",
      "epoch:375 train_loss:0.4565 val_loss:0.5526 val_accuracy:0.8347\n",
      "epoch:376 train_loss:0.4626 val_loss:0.5467 val_accuracy:0.8361\n",
      "epoch:377 train_loss:0.4864 val_loss:0.6060 val_accuracy:0.8227\n",
      "epoch:378 train_loss:0.5057 val_loss:0.6612 val_accuracy:0.8067\n",
      "epoch:379 train_loss:0.4764 val_loss:0.5400 val_accuracy:0.8355\n",
      "epoch:380 train_loss:0.4911 val_loss:0.5373 val_accuracy:0.8358\n",
      "epoch:381 train_loss:0.4588 val_loss:0.5363 val_accuracy:0.8368\n",
      "epoch:382 train_loss:0.4744 val_loss:0.5446 val_accuracy:0.8344\n",
      "epoch:383 train_loss:0.4546 val_loss:0.5434 val_accuracy:0.8360\n",
      "epoch:384 train_loss:0.4647 val_loss:0.5439 val_accuracy:0.8353\n",
      "epoch:385 train_loss:0.4598 val_loss:0.5397 val_accuracy:0.8369\n",
      "epoch:386 train_loss:0.4557 val_loss:0.5401 val_accuracy:0.8383\n",
      "epoch:387 train_loss:0.4430 val_loss:0.5397 val_accuracy:0.8388\n",
      "epoch:388 train_loss:0.4578 val_loss:0.5404 val_accuracy:0.8367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:389 train_loss:0.4511 val_loss:0.5439 val_accuracy:0.8371\n",
      "epoch:390 train_loss:0.4414 val_loss:0.5382 val_accuracy:0.8391\n",
      "epoch:391 train_loss:0.4601 val_loss:0.5405 val_accuracy:0.8385\n",
      "epoch:392 train_loss:0.4425 val_loss:0.5417 val_accuracy:0.8387\n",
      "epoch:393 train_loss:0.4678 val_loss:0.5390 val_accuracy:0.8390\n",
      "epoch:394 train_loss:0.4602 val_loss:0.5411 val_accuracy:0.8380\n",
      "epoch:395 train_loss:0.4374 val_loss:0.5386 val_accuracy:0.8393\n",
      "epoch:396 train_loss:0.4482 val_loss:0.5393 val_accuracy:0.8388\n",
      "epoch:397 train_loss:0.4244 val_loss:0.5476 val_accuracy:0.8363\n",
      "epoch:398 train_loss:0.4460 val_loss:0.5353 val_accuracy:0.8402\n",
      "epoch:399 train_loss:0.4437 val_loss:0.5347 val_accuracy:0.8377\n",
      "epoch:400 train_loss:0.4414 val_loss:0.5394 val_accuracy:0.8395\n",
      "epoch:401 train_loss:0.4383 val_loss:0.5363 val_accuracy:0.8394\n",
      "epoch:402 train_loss:0.4399 val_loss:0.5361 val_accuracy:0.8398\n",
      "epoch:403 train_loss:0.4371 val_loss:0.5394 val_accuracy:0.8383\n",
      "epoch:404 train_loss:0.4456 val_loss:0.5389 val_accuracy:0.8380\n",
      "epoch:405 train_loss:0.4499 val_loss:0.5407 val_accuracy:0.8392\n",
      "epoch:406 train_loss:0.4513 val_loss:0.5366 val_accuracy:0.8394\n",
      "epoch:407 train_loss:0.4455 val_loss:0.5383 val_accuracy:0.8404\n",
      "epoch:408 train_loss:0.4573 val_loss:0.5329 val_accuracy:0.8406\n",
      "epoch:409 train_loss:0.4275 val_loss:0.5311 val_accuracy:0.8409\n",
      "epoch:410 train_loss:0.4362 val_loss:0.5374 val_accuracy:0.8416\n",
      "epoch:411 train_loss:0.4463 val_loss:0.5346 val_accuracy:0.8401\n",
      "epoch:412 train_loss:0.4460 val_loss:0.5314 val_accuracy:0.8415\n",
      "epoch:413 train_loss:0.4337 val_loss:0.5389 val_accuracy:0.8396\n",
      "epoch:414 train_loss:0.4328 val_loss:0.5455 val_accuracy:0.8369\n",
      "epoch:415 train_loss:0.4428 val_loss:0.5362 val_accuracy:0.8381\n",
      "epoch:416 train_loss:0.4399 val_loss:0.5269 val_accuracy:0.8431\n",
      "epoch:417 train_loss:0.4282 val_loss:0.5414 val_accuracy:0.8373\n",
      "epoch:418 train_loss:0.4350 val_loss:0.5339 val_accuracy:0.8416\n",
      "epoch:419 train_loss:0.4340 val_loss:0.5285 val_accuracy:0.8421\n",
      "epoch:420 train_loss:0.4499 val_loss:0.5327 val_accuracy:0.8412\n",
      "epoch:421 train_loss:0.4350 val_loss:0.5330 val_accuracy:0.8409\n",
      "epoch:422 train_loss:0.4411 val_loss:0.5372 val_accuracy:0.8407\n",
      "epoch:423 train_loss:0.4309 val_loss:0.5406 val_accuracy:0.8385\n",
      "epoch:424 train_loss:0.4623 val_loss:0.5524 val_accuracy:0.8325\n",
      "epoch:425 train_loss:0.4383 val_loss:0.5243 val_accuracy:0.8400\n",
      "epoch:426 train_loss:0.4370 val_loss:0.5218 val_accuracy:0.8422\n",
      "epoch:427 train_loss:0.4317 val_loss:0.5218 val_accuracy:0.8435\n",
      "epoch:428 train_loss:0.4303 val_loss:0.5237 val_accuracy:0.8428\n",
      "epoch:429 train_loss:0.4291 val_loss:0.5346 val_accuracy:0.8396\n",
      "epoch:430 train_loss:0.4357 val_loss:0.5209 val_accuracy:0.8443\n",
      "epoch:431 train_loss:0.4437 val_loss:0.5204 val_accuracy:0.8438\n",
      "epoch:432 train_loss:0.4318 val_loss:0.5247 val_accuracy:0.8423\n",
      "epoch:433 train_loss:0.4505 val_loss:0.5287 val_accuracy:0.8416\n",
      "epoch:434 train_loss:0.4392 val_loss:0.5232 val_accuracy:0.8423\n",
      "epoch:435 train_loss:0.4301 val_loss:0.5259 val_accuracy:0.8428\n",
      "epoch:436 train_loss:0.4417 val_loss:0.5206 val_accuracy:0.8427\n",
      "epoch:437 train_loss:0.4138 val_loss:0.5149 val_accuracy:0.8457\n",
      "epoch:438 train_loss:0.4178 val_loss:0.5185 val_accuracy:0.8452\n",
      "epoch:439 train_loss:0.4368 val_loss:0.5192 val_accuracy:0.8444\n",
      "epoch:440 train_loss:0.4058 val_loss:0.5188 val_accuracy:0.8448\n",
      "epoch:441 train_loss:0.4061 val_loss:0.5181 val_accuracy:0.8462\n",
      "epoch:442 train_loss:0.4164 val_loss:0.5249 val_accuracy:0.8430\n",
      "epoch:443 train_loss:0.4341 val_loss:0.5313 val_accuracy:0.8418\n",
      "epoch:444 train_loss:0.4153 val_loss:0.5198 val_accuracy:0.8440\n",
      "epoch:445 train_loss:0.4224 val_loss:0.5192 val_accuracy:0.8448\n",
      "epoch:446 train_loss:0.4226 val_loss:0.5210 val_accuracy:0.8433\n",
      "epoch:447 train_loss:0.4112 val_loss:0.5183 val_accuracy:0.8446\n",
      "epoch:448 train_loss:0.4215 val_loss:0.5175 val_accuracy:0.8462\n",
      "epoch:449 train_loss:0.4018 val_loss:0.5199 val_accuracy:0.8433\n",
      "epoch:450 train_loss:0.4282 val_loss:0.5170 val_accuracy:0.8458\n",
      "epoch:451 train_loss:0.4074 val_loss:0.5187 val_accuracy:0.8445\n",
      "epoch:452 train_loss:0.4026 val_loss:0.5288 val_accuracy:0.8408\n",
      "epoch:453 train_loss:0.4205 val_loss:0.5151 val_accuracy:0.8458\n",
      "epoch:454 train_loss:0.4022 val_loss:0.5125 val_accuracy:0.8464\n",
      "epoch:455 train_loss:0.4228 val_loss:0.5144 val_accuracy:0.8461\n",
      "epoch:456 train_loss:0.4278 val_loss:0.5383 val_accuracy:0.8414\n",
      "epoch:457 train_loss:0.4157 val_loss:0.5193 val_accuracy:0.8442\n",
      "epoch:458 train_loss:0.4097 val_loss:0.5139 val_accuracy:0.8472\n",
      "epoch:459 train_loss:0.4095 val_loss:0.5145 val_accuracy:0.8458\n",
      "epoch:460 train_loss:0.4031 val_loss:0.5164 val_accuracy:0.8467\n",
      "epoch:461 train_loss:0.4204 val_loss:0.5216 val_accuracy:0.8475\n",
      "epoch:462 train_loss:0.4237 val_loss:0.5166 val_accuracy:0.8459\n",
      "epoch:463 train_loss:0.4043 val_loss:0.5149 val_accuracy:0.8459\n",
      "epoch:464 train_loss:0.4150 val_loss:0.5111 val_accuracy:0.8467\n",
      "epoch:465 train_loss:0.4124 val_loss:0.5130 val_accuracy:0.8473\n",
      "epoch:466 train_loss:0.4124 val_loss:0.5129 val_accuracy:0.8469\n",
      "epoch:467 train_loss:0.4151 val_loss:0.5044 val_accuracy:0.8476\n",
      "epoch:468 train_loss:0.4034 val_loss:0.5094 val_accuracy:0.8470\n",
      "epoch:469 train_loss:0.3976 val_loss:0.5359 val_accuracy:0.8400\n",
      "epoch:470 train_loss:0.4025 val_loss:0.5227 val_accuracy:0.8423\n",
      "epoch:471 train_loss:0.4127 val_loss:0.5072 val_accuracy:0.8477\n",
      "epoch:472 train_loss:0.4073 val_loss:0.5235 val_accuracy:0.8460\n",
      "epoch:473 train_loss:0.4168 val_loss:0.5353 val_accuracy:0.8404\n",
      "epoch:474 train_loss:0.4101 val_loss:0.5097 val_accuracy:0.8462\n",
      "epoch:475 train_loss:0.4043 val_loss:0.5252 val_accuracy:0.8429\n",
      "epoch:476 train_loss:0.4120 val_loss:0.5173 val_accuracy:0.8440\n",
      "epoch:477 train_loss:0.3975 val_loss:0.5103 val_accuracy:0.8474\n",
      "epoch:478 train_loss:0.3956 val_loss:0.5107 val_accuracy:0.8490\n",
      "epoch:479 train_loss:0.4036 val_loss:0.5092 val_accuracy:0.8460\n",
      "epoch:480 train_loss:0.4042 val_loss:0.5213 val_accuracy:0.8433\n",
      "epoch:481 train_loss:0.4037 val_loss:0.5099 val_accuracy:0.8469\n",
      "epoch:482 train_loss:0.4055 val_loss:0.5053 val_accuracy:0.8500\n",
      "epoch:483 train_loss:0.4040 val_loss:0.5054 val_accuracy:0.8483\n",
      "epoch:484 train_loss:0.3893 val_loss:0.5078 val_accuracy:0.8477\n",
      "epoch:485 train_loss:0.4050 val_loss:0.5029 val_accuracy:0.8492\n",
      "epoch:486 train_loss:0.4103 val_loss:0.5057 val_accuracy:0.8483\n",
      "epoch:487 train_loss:0.4135 val_loss:0.5118 val_accuracy:0.8453\n",
      "epoch:488 train_loss:0.3926 val_loss:0.5122 val_accuracy:0.8470\n",
      "epoch:489 train_loss:0.3989 val_loss:0.5181 val_accuracy:0.8455\n",
      "epoch:490 train_loss:0.4044 val_loss:0.5251 val_accuracy:0.8441\n",
      "epoch:491 train_loss:1.2605 val_loss:2.2212 val_accuracy:0.5419\n",
      "epoch:492 train_loss:0.6360 val_loss:0.6228 val_accuracy:0.8041\n",
      "epoch:493 train_loss:0.5415 val_loss:0.5511 val_accuracy:0.8317\n",
      "epoch:494 train_loss:0.4799 val_loss:0.5303 val_accuracy:0.8380\n",
      "epoch:495 train_loss:0.4429 val_loss:0.5246 val_accuracy:0.8400\n",
      "epoch:496 train_loss:0.4436 val_loss:0.5145 val_accuracy:0.8433\n",
      "epoch:497 train_loss:0.4443 val_loss:0.5137 val_accuracy:0.8441\n",
      "epoch:498 train_loss:0.4304 val_loss:0.5090 val_accuracy:0.8448\n",
      "epoch:499 train_loss:0.4388 val_loss:0.5123 val_accuracy:0.8431\n",
      "epoch:500 train_loss:0.4159 val_loss:0.5151 val_accuracy:0.8426\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chainer import Variable\n",
    "from chainer.backends import cuda\n",
    "from chainer.cuda import to_cpu\n",
    "from chainer.dataset import concat_examples\n",
    "\n",
    "max_epoch = 500\n",
    "bprop_len = 200\n",
    "\n",
    "batchno = 0\n",
    "while train_iter.epoch < max_epoch:\n",
    "\n",
    "    # ---------- 学習の1イテレーション ----------\n",
    "    train_batch = train_iter.next() # batchsize x (framelen x 26 , framelen)\n",
    "    #batchno += 1\n",
    "    #print(\"batch no {}\".format(batchno))\n",
    "    loss = 0\n",
    "    \n",
    "    xs = []\n",
    "    ts = []\n",
    "    for X, t in train_batch:\n",
    "        if gpu_id >= 0:\n",
    "            X = cuda.to_gpu(X, gpu_id)\n",
    "            t = cuda.to_gpu(t, gpu_id)\n",
    "        t = t[:,np.newaxis]\n",
    "        xs.append(chainer.Variable(X.astype(dtype=xp.float32)))\n",
    "        ts.append(chainer.Variable(t))\n",
    "\n",
    "    # 予測値の計算\n",
    "    ys = net(xs)\n",
    "\n",
    "    # ロスの計算\n",
    "    ts = F.concat(ts, axis=0)\n",
    "    ts = ts.reshape(ts.shape[0],)\n",
    "    loss = F.softmax_cross_entropy(ys, ts)\n",
    "\n",
    "    # 勾配の計算\n",
    "    net.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    # バッチ単位で古い記憶を削除し、計算コストを削減する。\n",
    "    loss.unchain_backward()\n",
    "\n",
    "    # パラメータの更新\n",
    "    optimizer.update()\n",
    "    # --------------- ここまで ----------------\n",
    "\n",
    "    # 1エポック終了ごとにValidationデータに対する予測精度を測って、\n",
    "    # モデルの汎化性能が向上していることをチェックしよう\n",
    "    if train_iter.is_new_epoch:  # 1 epochが終わったら\n",
    "        batchno = 0\n",
    "\n",
    "        # ロスの表示\n",
    "        print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "            train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "\n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "\n",
    "        xs = []\n",
    "        ts = []\n",
    "        for X, t in test:\n",
    "            if gpu_id >= 0:\n",
    "                X = cuda.to_gpu(X, gpu_id)\n",
    "                t = cuda.to_gpu(t, gpu_id)\n",
    "            t = t[:,np.newaxis]\n",
    "            xs.append(chainer.Variable(X.astype(dtype=xp.float32)))\n",
    "            ts.append(chainer.Variable(t))\n",
    "        \n",
    "        # Validationデータをforward\n",
    "        with chainer.using_config('train', False), \\\n",
    "                chainer.using_config('enable_backprop', False):\n",
    "            ys = net(xs)\n",
    "\n",
    "        ts = F.concat(ts, axis=0)\n",
    "        ts = ts.reshape(ts.shape[0],)\n",
    "        # ロスを計算\n",
    "        loss_valid = F.softmax_cross_entropy(ys, ts)\n",
    "        valid_losses.append(to_cpu(loss_valid.array))\n",
    "\n",
    "        # 精度を計算\n",
    "        accuracy = F.accuracy(ys, ts)\n",
    "        accuracy.to_cpu()\n",
    "        valid_accuracies.append(accuracy.array)\n",
    "\n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            np.mean(valid_losses), np.mean(valid_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu_id >= 0:\n",
    "    net.to_gpu(gpu_id)\n",
    "\n",
    "# 1つ目のテストデータを取り出します\n",
    "X, t = test[0]  #  tは使わない\n",
    "\n",
    "# print('ミニバッチの形にしたあと：', x.shape)\n",
    "\n",
    "xs = []\n",
    "if gpu_id >= 0:\n",
    "    X = cuda.to_gpu(X, gpu_id)\n",
    "xs.append(chainer.Variable(X.astype(dtype=xp.float32)))\n",
    "\n",
    "# Validationデータをforward\n",
    "with chainer.using_config('train', False), \\\n",
    "        chainer.using_config('enable_backprop', False):\n",
    "    y_test = net(xs)\n",
    "\n",
    "# Variable形式で出てくるので中身を取り出す\n",
    "y_test = y_test.array\n",
    "\n",
    "# 結果をCPUに送る\n",
    "y_test = to_cpu(y_test)\n",
    "\n",
    "# 予測確率の最大値のインデックスを見る\n",
    "pred_label = y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/hiroki/conda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch', 'ch',\n",
       "       'ch', 'ch', 'ch', 'ch', 'ch', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 's', 's', 's', 's', 's', 's', 's', 's', 's',\n",
       "       's', 'a', 'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'a', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'u', 'u', 'u', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a',\n",
       "       'a', 'a', 'a', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'y', 'y', 'y', 'y', 'y', 'y', 'y', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'n',\n",
       "       'n', 'n', 'n', 'n', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e',\n",
       "       'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'k', 'k',\n",
       "       'k', 'k', 'k', 'i', 'i', 'i', 'i', 'i', 'n', 'n', 'n', 'n', 'n',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'y', 'y', 'y', 'y', 'y', 'y', 'y',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o',\n",
       "       'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'o', 'o', 'o', 'o', 'o', 'n', 'n', 'n',\n",
       "       'n', 'n', 'o', 'o', 'o', 'o', 'g', 'g', 'g', 'g', 'g', 'g', 'g',\n",
       "       'g', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'i', 'i', 'i', 'i', 'i', 'i', 'n', 'n',\n",
       "       'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a',\n",
       "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'r', 'r', 'r', 'r', 'r', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'u', 'u', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil'], dtype='<U3')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "with open('../phones') as f:\n",
    "    phones = f.read().splitlines()\n",
    "le = LabelEncoder()\n",
    "le.fit(phones)\n",
    "le.inverse_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/hiroki/conda/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil', 'sil',\n",
       "       'ch', 'ch', 't', 't', 't', 't', 't', 'ch', 'ch', 'ch', 'ch', 'ch',\n",
       "       'ch', 'ch', 'ch', 'ch', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'e',\n",
       "       'a', 'a', 'a', 'a', 'a', 'n', 'n', 'n', 'n', 'n', 'a', 'a', 'a',\n",
       "       'a', 'a', 'a', 'a', 'a', 'r', 'u', 'u', 'u', 'u', 'u', 'u', 'u',\n",
       "       'u', 'N', 'N', 'n', 'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a',\n",
       "       'a', 'a', 'g', 'g', 'g', 'n', 'n', 'n', 'g', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'y', 'y', 'y', 'a', 'a', 'a',\n",
       "       'a', 'a', 'a', 'a', 'r', 'r', 'n', 'n', 'n', 'n', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'g', 'g',\n",
       "       'g', 'n', 'n', 'g', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'k',\n",
       "       'k', 'k', 'k', 'k', 't', 't', 'Q', 'Q', 'Q', 'Q', 't', 't', 'k',\n",
       "       'k', 'k', 'ky', 'i', 'i', 'i', 'n', 'n', 'n', 'n', 'n', 'n', 'n',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'y', 'y', 'y', 'y', 'y', 'y', 'o',\n",
       "       'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'n',\n",
       "       'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'o', 'o', 'o', 'o', 'o', 'o', 'n', 'n', 'n',\n",
       "       'n', 'o', 'o', 'o', 'o', 'o', 'N', 'N', 'N', 'N', 'N', 'n', 'n',\n",
       "       'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b',\n",
       "       'b', 'b', 'g', 'g', 'g', 'i', 'i', 'i', 'i', 'i', 'i', 'n', 'n',\n",
       "       'n', 'n', 'n', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'g',\n",
       "       'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'i', 'i', 'i', 'i', 'i',\n",
       "       'i', 'i', 'i', 'i', 'i', 'r', 'r', 'r', 'r', 'u', 'u', 'u', 'u',\n",
       "       'u', 'u', 'u', 'u', 'u', 'u', 'sil', 'sil', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau',\n",
       "       'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau', 'pau'],\n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
